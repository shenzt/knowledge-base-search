#!/usr/bin/env python3
"""çŸ¥è¯†åº“ç´¢å¼•æ„å»ºå·¥å…·ã€‚

æŒ‰ Markdown æ ‡é¢˜è¯­ä¹‰åˆ†å—ï¼Œæ³¨å…¥ section_path åˆ° Qdrant payloadã€‚
æ”¯æŒå•æ–‡ä»¶ã€å…¨é‡é‡å»ºã€å¢é‡æ›´æ–°ã€‚

ç”¨æ³•:
  # ç´¢å¼•å•ä¸ªæ–‡ä»¶
  python scripts/index.py --file docs/runbook/redis.md

  # å…¨é‡é‡å»ºï¼ˆéå†æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ .mdï¼‰
  python scripts/index.py --full docs/

  # å¢é‡æ›´æ–°ï¼ˆåŸºäº git diffï¼‰
  python scripts/index.py --incremental

  # ä» stdin æ¥æ”¶ chunks JSON
  echo '[{"doc_id":"abc","chunk_id":"abc-000","text":"...","metadata":{}}]' | python scripts/index.py

  # åˆ é™¤æŸä¸ªæ–‡æ¡£çš„æ‰€æœ‰ chunks
  python scripts/index.py --delete --doc-id abc12345

  # æŸ¥çœ‹ç´¢å¼•çŠ¶æ€
  python scripts/index.py --status
"""

import argparse
import hashlib
import json
import logging
import os
import re
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

import frontmatter
import numpy as np
from FlagEmbedding import BGEM3FlagModel
from qdrant_client import QdrantClient, models

logging.basicConfig(level=logging.INFO, format="%(message)s")
log = logging.getLogger(__name__)

QDRANT_URL = os.environ.get("QDRANT_URL", "http://localhost:6333")
COLLECTION = os.environ.get("COLLECTION_NAME", "knowledge-base")
MODEL_NAME = os.environ.get("BGE_M3_MODEL", "BAAI/bge-m3")
MAX_CHUNK_CHARS = 3200

_model = None


def get_model() -> BGEM3FlagModel:
    """å»¶è¿ŸåŠ è½½ BGE-M3 æ¨¡å‹ã€‚"""
    global _model
    if _model is None:
        log.info(f"åŠ è½½æ¨¡å‹ {MODEL_NAME}...")
        _model = BGEM3FlagModel(MODEL_NAME, use_fp16=True)
    return _model


def get_qdrant() -> QdrantClient:
    return QdrantClient(url=QDRANT_URL)


def ensure_collection(client: QdrantClient) -> None:
    """ç¡®ä¿ collection å­˜åœ¨ã€‚"""
    collections = [c.name for c in client.get_collections().collections]
    if COLLECTION not in collections:
        log.info(f"åˆ›å»º collection: {COLLECTION}")
        client.create_collection(
            collection_name=COLLECTION,
            vectors_config={
                "dense": models.VectorParams(size=1024, distance=models.Distance.COSINE),
            },
            sparse_vectors_config={
                "sparse": models.SparseVectorParams(),
            },
        )


# â”€â”€ æ ‡é¢˜åˆ†å— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _clean_hugo_shortcodes(content: str) -> str:
    """æ¸…ç† Hugo shortcodesï¼Œä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬ã€‚"""
    # {{< glossary_tooltip text="containers" term_id="container" >}} â†’ containers
    content = re.sub(
        r'\{\{<\s*glossary_tooltip\s+text="([^"]+)"[^>]*>\}\}',
        r'\1', content)
    # {{< glossary_tooltip term_id="node" >}} â†’ node
    content = re.sub(
        r'\{\{<\s*glossary_tooltip\s+term_id="([^"]+)"[^>]*>\}\}',
        r'\1', content)
    # {{< note >}} ... {{< /note >}} â†’ keep content
    content = re.sub(r'\{\{[<%]\s*/?\s*note\s*[%>]\}\}', '', content)
    # {{< warning >}} ... {{< /warning >}} â†’ keep content
    content = re.sub(r'\{\{[<%]\s*/?\s*warning\s*[%>]\}\}', '', content)
    # {{< feature-state ... >}} â†’ remove
    content = re.sub(r'\{\{<\s*feature-state[^>]*>\}\}', '', content)
    # {{% code_sample file="..." %}} â†’ [code sample: ...]
    content = re.sub(
        r'\{\{%\s*code_sample\s+file="([^"]+)"\s*%\}\}',
        r'[code: \1]', content)
    # <!-- overview --> etc HTML comments â†’ remove
    content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)
    # Any remaining shortcodes â†’ remove
    content = re.sub(r'\{\{[<%][^}]*[%>]\}\}', '', content)
    # {{< relref "..." >}} inside links â†’ keep link text
    content = re.sub(r'\{\{<\s*relref\s+"[^"]*"\s*>\}\}', '', content)
    return content

def _find_code_fence_ranges(content: str) -> list[tuple[int, int]]:
    """æ‰¾å‡ºæ‰€æœ‰ä»£ç å›´æ  (```) çš„èŒƒå›´ï¼Œè¿”å› [(start, end), ...]ã€‚"""
    fence_re = re.compile(r'^```', re.MULTILINE)
    ranges = []
    matches = list(fence_re.finditer(content))
    for i in range(0, len(matches) - 1, 2):
        ranges.append((matches[i].start(), matches[i + 1].end()))
    return ranges


def _in_code_fence(pos: int, ranges: list[tuple[int, int]]) -> bool:
    """åˆ¤æ–­æŸä¸ªä½ç½®æ˜¯å¦åœ¨ä»£ç å›´æ å†…ã€‚"""
    return any(start <= pos <= end for start, end in ranges)


def split_by_headings(content: str) -> list[dict]:
    """æŒ‰ Markdown æ ‡é¢˜åˆ‡åˆ†ï¼Œä¿ç•™ section_path å±‚çº§ã€‚è·³è¿‡ä»£ç å—ä¸­çš„ #ã€‚

    è¿”å›: [{"text": "...", "section_path": "æ•…éšœæ¢å¤ > æ‰‹åŠ¨æ¢å¤ > ç¡®è®¤æ–° Master"}]
    """
    heading_re = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)
    code_ranges = _find_code_fence_ranges(content)
    sections = []
    headings_stack: list[tuple[int, str]] = []  # [(level, title), ...]
    last_pos = 0

    for match in heading_re.finditer(content):
        # è·³è¿‡ä»£ç å—ä¸­çš„ #
        if _in_code_fence(match.start(), code_ranges):
            continue

        # ä¿å­˜ä¸Šä¸€æ®µ
        if last_pos < match.start():
            text = content[last_pos:match.start()].strip()
            if text:
                path = " > ".join(h[1] for h in headings_stack)
                sections.append({"text": text, "section_path": path})

        # æ›´æ–°æ ‡é¢˜æ ˆ
        level = len(match.group(1))
        title = match.group(2).strip()
        # å¼¹å‡ºåŒçº§æˆ–æ›´ä½çº§çš„æ ‡é¢˜
        while headings_stack and headings_stack[-1][0] >= level:
            headings_stack.pop()
        headings_stack.append((level, title))
        last_pos = match.end()

    # æœ€åä¸€æ®µ
    if last_pos < len(content):
        text = content[last_pos:].strip()
        if text:
            path = " > ".join(h[1] for h in headings_stack)
            sections.append({"text": text, "section_path": path})

    return sections if sections else [{"text": content.strip(), "section_path": ""}]


def merge_small_sections(sections: list[dict], max_chars: int = MAX_CHUNK_CHARS) -> list[dict]:
    """åˆå¹¶è¿‡çŸ­çš„ç›¸é‚» sectionï¼ˆåŒä¸€ section_path å‰ç¼€ä¸‹ï¼‰ã€‚"""
    if not sections:
        return sections

    merged = []
    buf_text = ""
    buf_path = ""

    for sec in sections:
        if not buf_text:
            buf_text = sec["text"]
            buf_path = sec["section_path"]
        elif len(buf_text) + len(sec["text"]) < max_chars and _same_parent(buf_path, sec["section_path"]):
            buf_text = buf_text + "\n\n" + sec["text"]
            buf_path = sec["section_path"]  # ç”¨æœ€æ–°çš„ path
        else:
            merged.append({"text": buf_text, "section_path": buf_path})
            buf_text = sec["text"]
            buf_path = sec["section_path"]

    if buf_text:
        merged.append({"text": buf_text, "section_path": buf_path})

    return merged


def _same_parent(path_a: str, path_b: str) -> bool:
    """åˆ¤æ–­ä¸¤ä¸ª section_path æ˜¯å¦æœ‰ç›¸åŒçš„çˆ¶çº§ã€‚"""
    parts_a = path_a.split(" > ")[:-1]
    parts_b = path_b.split(" > ")[:-1]
    return parts_a == parts_b


# â”€â”€ ç´¢å¼•æ ¸å¿ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def index_chunks(chunks: list[dict], batch_size: int = 256) -> None:
    """å°† chunks ç¼–ç ä¸ºå‘é‡å¹¶å†™å…¥ Qdrantã€‚æ”¯æŒå¤§æ‰¹é‡åˆ†æ‰¹ç¼–ç ã€‚"""
    if not chunks:
        log.info("æ²¡æœ‰ chunks éœ€è¦ç´¢å¼•")
        return

    model = get_model()
    client = get_qdrant()
    ensure_collection(client)

    # ç¼–ç æ—¶æ‹¼æ¥ title + textï¼Œæå‡çŸ­æ–‡æ¡£çš„è¯­ä¹‰ä¿¡å·
    # å­˜å‚¨çš„ payload.text ä¿æŒåŸå§‹å†…å®¹ä¸å˜
    encode_texts = []
    for c in chunks:
        title = c.get("metadata", {}).get("title", "")
        text = c["text"]
        encode_texts.append(f"{title}\n{text}" if title else text)

    log.info(f"ç¼–ç  {len(encode_texts)} ä¸ª chunksï¼ˆbatch_size={batch_size}ï¼‰...")
    output = model.encode(encode_texts, return_dense=True, return_sparse=True,
                          batch_size=batch_size)

    points = []
    for i, chunk in enumerate(chunks):
        sparse = output["lexical_weights"][i]
        point_id = hashlib.md5(chunk["chunk_id"].encode()).hexdigest()

        payload = {
            "doc_id": chunk["doc_id"],
            "chunk_id": chunk["chunk_id"],
            "text": chunk["text"],
        }
        payload.update(chunk.get("metadata", {}))

        points.append(models.PointStruct(
            id=point_id,
            vector={
                "dense": output["dense_vecs"][i].tolist(),
                "sparse": models.SparseVector(
                    indices=list(map(int, sparse.keys())),
                    values=list(sparse.values()),
                ),
            },
            payload=payload,
        ))

    # åˆ†æ‰¹ upsertï¼ˆQdrant å•æ¬¡ä¸Šé™çº¦ 1000 ç‚¹ï¼‰
    UPSERT_BATCH = 500
    for start in range(0, len(points), UPSERT_BATCH):
        batch = points[start:start + UPSERT_BATCH]
        client.upsert(collection_name=COLLECTION, points=batch)
        if len(points) > UPSERT_BATCH:
            log.info(f"  upsert {start + len(batch)}/{len(points)}")

    log.info(f"âœ… å·²ç´¢å¼• {len(points)} ä¸ª chunks")


def delete_doc(doc_id: str) -> None:
    """åˆ é™¤æŸä¸ªæ–‡æ¡£çš„æ‰€æœ‰ chunksã€‚"""
    client = get_qdrant()
    client.delete(
        collection_name=COLLECTION,
        points_selector=models.FilterSelector(
            filter=models.Filter(must=[
                models.FieldCondition(key="doc_id", match=models.MatchValue(value=doc_id))
            ])
        ),
    )
    log.info(f"âœ… å·²åˆ é™¤ doc_id={doc_id} çš„æ‰€æœ‰ chunks")


def delete_by_source_repo(repo_url: str) -> None:
    """æŒ‰ source_repo å­—æ®µæ‰¹é‡åˆ é™¤æŸä¸ªä»“åº“çš„æ‰€æœ‰ chunksã€‚"""
    client = get_qdrant()
    # å…ˆç»Ÿè®¡æ•°é‡
    count_result = client.count(
        collection_name=COLLECTION,
        count_filter=models.Filter(must=[
            models.FieldCondition(key="source_repo", match=models.MatchValue(value=repo_url))
        ]),
    )
    n = count_result.count
    if n == 0:
        log.info(f"source_repo={repo_url} æ—  chunksï¼Œè·³è¿‡åˆ é™¤")
        return

    client.delete(
        collection_name=COLLECTION,
        points_selector=models.FilterSelector(
            filter=models.Filter(must=[
                models.FieldCondition(key="source_repo", match=models.MatchValue(value=repo_url))
            ])
        ),
    )
    log.info(f"âœ… å·²åˆ é™¤ source_repo={repo_url} çš„ {n} ä¸ª chunks")


def _stable_doc_id(filepath: str) -> str:
    """ä»æ–‡ä»¶è·¯å¾„ç”Ÿæˆç¨³å®šçš„ doc_idï¼ˆåŸºäºç›¸å¯¹è·¯å¾„ï¼Œè·¨æœºå™¨ä¸€è‡´ï¼‰ã€‚"""
    # å»æ‰å¸¸è§å‰ç¼€ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„è·¯å¾„éƒ¨åˆ†
    p = filepath
    for prefix in ["tests/fixtures/kb-sources/k8s-website/content/en/docs/",
                    "tests/fixtures/kb-sources/redis-docs/content/",
                    "docs/"]:
        if prefix in p:
            p = p[p.index(prefix) + len(prefix):]
            break
    # ç”¨è·¯å¾„çš„ md5 å‰ 8 ä½
    return hashlib.md5(p.encode()).hexdigest()[:8]


def parse_file(filepath: str) -> list[dict]:
    """è§£æå•ä¸ª Markdown æ–‡ä»¶ä¸º chunksï¼ˆä¸ç¼–ç ã€ä¸å†™å…¥ Qdrantï¼‰ã€‚"""
    post = frontmatter.load(filepath)
    doc_id = post.metadata.get("id", _stable_doc_id(filepath))
    title = post.metadata.get("title", os.path.basename(filepath))

    content = _clean_hugo_shortcodes(post.content)
    sections = split_by_headings(content)
    sections = merge_small_sections(sections)

    chunk_data = []
    for i, sec in enumerate(sections):
        chunk_data.append({
            "doc_id": doc_id,
            "chunk_id": f"{doc_id}-{i:03d}",
            "text": sec["text"],
            "metadata": {
                "path": filepath,
                "title": title,
                "section_path": sec["section_path"],
                "chunk_index": i,
                "confidence": post.metadata.get("confidence", "unknown"),
                "tags": post.metadata.get("tags", []),
                "source_repo": post.metadata.get("source_repo", ""),
                "source_path": post.metadata.get("source_path", ""),
                "source_commit": post.metadata.get("source_commit", ""),
            },
        })
    return chunk_data


def index_file(filepath: str) -> int:
    """ç´¢å¼•å•ä¸ª Markdown æ–‡ä»¶ï¼ˆè§£æ + ç¼–ç  + å†™å…¥ï¼‰ã€‚è¿”å› chunk æ•°ã€‚"""
    post = frontmatter.load(filepath)
    doc_id = post.metadata.get("id", _stable_doc_id(filepath))

    # å…ˆåˆ é™¤æ—§ chunks
    try:
        delete_doc(doc_id)
    except Exception:
        pass

    chunk_data = parse_file(filepath)
    index_chunks(chunk_data)
    return len(chunk_data)


# â”€â”€ å…¨é‡ / å¢é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def index_full(docs_dir: str) -> None:
    """å…¨é‡é‡å»ºï¼šå…ˆæ”¶é›†æ‰€æœ‰ chunksï¼Œå†ä¸€æ¬¡æ€§æ‰¹é‡ç¼–ç  + å†™å…¥ã€‚"""
    md_files = sorted(Path(docs_dir).rglob("*.md"))
    if not md_files:
        log.info(f"ç›®å½• {docs_dir} ä¸‹æ²¡æœ‰ .md æ–‡ä»¶")
        return

    log.info(f"å…¨é‡ç´¢å¼•: {len(md_files)} ä¸ªæ–‡ä»¶ ({docs_dir})")

    # Phase 1: æ”¶é›†æ‰€æœ‰ doc_id ç”¨äºæ‰¹é‡åˆ é™¤
    doc_ids: set[str] = set()
    all_chunks: list[dict] = []
    errors = 0

    for f in md_files:
        try:
            chunks = parse_file(str(f))
            if chunks:
                doc_ids.add(chunks[0]["doc_id"])
                all_chunks.extend(chunks)
                log.info(f"  ğŸ“„ {f} â†’ {len(chunks)} chunks")
        except Exception as e:
            log.error(f"  âŒ {f}: {e}")
            errors += 1

    log.info(f"è§£æå®Œæˆ: {len(md_files) - errors} æ–‡ä»¶, {len(all_chunks)} chunks")

    if not all_chunks:
        return

    # Phase 2: æ‰¹é‡åˆ é™¤æ—§ chunks
    client = get_qdrant()
    ensure_collection(client)
    for doc_id in doc_ids:
        try:
            client.delete(
                collection_name=COLLECTION,
                points_selector=models.FilterSelector(
                    filter=models.Filter(must=[
                        models.FieldCondition(key="doc_id", match=models.MatchValue(value=doc_id))
                    ])
                ),
            )
        except Exception:
            pass
    log.info(f"å·²æ¸…ç† {len(doc_ids)} ä¸ªæ—§æ–‡æ¡£çš„ chunks")

    # Phase 3: ä¸€æ¬¡æ€§æ‰¹é‡ç¼–ç  + å†™å…¥
    index_chunks(all_chunks)
    log.info(f"âœ… å…¨é‡ç´¢å¼•å®Œæˆ: {len(md_files) - errors} æ–‡ä»¶, {len(all_chunks)} chunks")


def index_incremental() -> None:
    """å¢é‡æ›´æ–°ï¼šåŸºäº git diff æ‰¾å‡ºå˜æ›´çš„ .md æ–‡ä»¶ã€‚"""
    try:
        # æ‰¾å‡ºæœ€è¿‘ä¸€æ¬¡ commit åˆ°å·¥ä½œåŒºçš„å˜æ›´
        result = subprocess.run(
            ["git", "diff", "--name-only", "HEAD", "--", "*.md"],
            capture_output=True, text=True, check=True,
        )
        changed = [f.strip() for f in result.stdout.strip().split("\n") if f.strip()]

        # ä¹ŸåŒ…æ‹¬æœªè·Ÿè¸ªçš„æ–°æ–‡ä»¶
        result2 = subprocess.run(
            ["git", "ls-files", "--others", "--exclude-standard", "--", "*.md"],
            capture_output=True, text=True, check=True,
        )
        new_files = [f.strip() for f in result2.stdout.strip().split("\n") if f.strip()]

        all_changed = list(set(changed + new_files))
    except subprocess.CalledProcessError:
        log.error("git å‘½ä»¤å¤±è´¥ï¼Œè¯·ç¡®è®¤åœ¨ git ä»“åº“ä¸­è¿è¡Œ")
        return

    if not all_changed:
        log.info("æ²¡æœ‰å˜æ›´çš„ .md æ–‡ä»¶")
        return

    log.info(f"å¢é‡ç´¢å¼•: {len(all_changed)} ä¸ªå˜æ›´æ–‡ä»¶")
    total_chunks = 0
    for f in all_changed:
        if not os.path.exists(f):
            # æ–‡ä»¶è¢«åˆ é™¤ï¼Œå°è¯•åˆ é™¤ç´¢å¼•
            doc_id = hashlib.md5(f.encode()).hexdigest()[:8]
            try:
                delete_doc(doc_id)
                log.info(f"  ğŸ—‘ï¸ {f} (å·²åˆ é™¤)")
            except Exception:
                pass
            continue
        try:
            n = index_file(f)
            total_chunks += n
            log.info(f"  {f} â†’ {n} chunks")
        except Exception as e:
            log.error(f"  âŒ {f}: {e}")

    log.info(f"âœ… å¢é‡ç´¢å¼•å®Œæˆ: {len(all_changed)} æ–‡ä»¶, {total_chunks} chunks")


# â”€â”€ çŠ¶æ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def show_status() -> None:
    """æ˜¾ç¤ºç´¢å¼•çŠ¶æ€ã€‚"""
    client = get_qdrant()
    try:
        info = client.get_collection(COLLECTION)
        log.info(f"Collection: {COLLECTION}")
        log.info(f"  å‘é‡æ•°: {info.points_count}")
        log.info(f"  çŠ¶æ€: {info.status}")

        # æŒ‰ doc_id ç»Ÿè®¡
        scroll_result = client.scroll(
            collection_name=COLLECTION,
            limit=1000,
            with_payload=["doc_id", "path", "title", "section_path"],
        )
        docs: dict[str, dict] = {}
        for point in scroll_result[0]:
            doc_id = point.payload.get("doc_id", "unknown")
            if doc_id not in docs:
                docs[doc_id] = {
                    "path": point.payload.get("path", ""),
                    "title": point.payload.get("title", ""),
                    "chunks": 0,
                    "has_section_path": False,
                }
            docs[doc_id]["chunks"] += 1
            if point.payload.get("section_path"):
                docs[doc_id]["has_section_path"] = True

        log.info(f"  æ–‡æ¡£æ•°: {len(docs)}")
        for doc_id, info_d in sorted(docs.items(), key=lambda x: x[1]["path"]):
            sp_tag = "ğŸ“‘" if info_d["has_section_path"] else "ğŸ“„"
            log.info(f"    {sp_tag} {info_d['path']} ({info_d['chunks']} chunks) [{doc_id}]")
    except Exception:
        log.info(f"Collection '{COLLECTION}' ä¸å­˜åœ¨ï¼Œè¿è¡Œç´¢å¼•å‘½ä»¤åˆ›å»º")


def drop_collection() -> None:
    """åˆ é™¤æ•´ä¸ª collectionã€‚"""
    client = get_qdrant()
    try:
        client.delete_collection(COLLECTION)
        log.info(f"âœ… å·²åˆ é™¤ collection: {COLLECTION}")
    except Exception:
        log.info(f"Collection '{COLLECTION}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")


# â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    parser = argparse.ArgumentParser(description="çŸ¥è¯†åº“ç´¢å¼•å·¥å…·")
    parser.add_argument("--file", help="ç´¢å¼•å•ä¸ª Markdown æ–‡ä»¶")
    parser.add_argument("--full", metavar="DIR", nargs="+", help="å…¨é‡é‡å»ºæŒ‡å®šç›®å½•ï¼ˆæ”¯æŒå¤šä¸ªï¼‰")
    parser.add_argument("--incremental", action="store_true", help="å¢é‡æ›´æ–°ï¼ˆåŸºäº git diffï¼‰")
    parser.add_argument("--delete", action="store_true", help="åˆ é™¤æ–‡æ¡£")
    parser.add_argument("--doc-id", help="è¦åˆ é™¤çš„ doc_id")
    parser.add_argument("--delete-by-repo", metavar="REPO_URL", help="æŒ‰ source_repo æ‰¹é‡åˆ é™¤æŸä»“åº“çš„æ‰€æœ‰ chunks")
    parser.add_argument("--drop", action="store_true", help="åˆ é™¤æ•´ä¸ª collectionï¼ˆæ¸…ç©ºç´¢å¼•ï¼‰")
    parser.add_argument("--status", action="store_true", help="æŸ¥çœ‹ç´¢å¼•çŠ¶æ€")
    args = parser.parse_args()

    if args.drop:
        drop_collection()
    elif args.status:
        show_status()
    elif args.delete_by_repo:
        delete_by_source_repo(args.delete_by_repo)
    elif args.delete and args.doc_id:
        delete_doc(args.doc_id)
    elif args.file:
        index_file(args.file)
    elif args.full:
        for d in args.full:
            index_full(d)
    elif args.incremental:
        index_incremental()
    else:
        # ä» stdin è¯»å– chunks JSON
        data = sys.stdin.read().strip()
        if data:
            chunks = json.loads(data)
            index_chunks(chunks)
        else:
            parser.print_help()


if __name__ == "__main__":
    main()
