#!/usr/bin/env python3
"""æ–‡æ¡£é¢„å¤„ç†ï¼šæ‰¹é‡ LLM åˆ†æï¼Œç”Ÿæˆ sidecar JSON å…ƒæ•°æ®ã€‚

ç”¨æ³•:
  python scripts/doc_preprocess.py --dir ../my-agent-kb/docs/redis-docs/
  python scripts/doc_preprocess.py --file docs/runbook/redis-failover.md
  python scripts/doc_preprocess.py --status ../my-agent-kb/docs/
  python scripts/doc_preprocess.py --dir ../my-agent-kb/docs/ --force
"""

import argparse
import hashlib
import json
import logging
import os
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from threading import Semaphore
from typing import Optional

from dotenv import load_dotenv
load_dotenv()

import frontmatter

logging.basicConfig(level=logging.INFO, format="%(message)s")
log = logging.getLogger(__name__)

SIDECAR_DIR = ".preprocess"
MAX_WORKERS = int(os.environ.get("PREPROCESS_WORKERS", "8"))
CACHE_FILE = "scripts/.preprocess_cache.json"
SCHEMA_VERSION = 1
PROMPT_VERSION = "doc_preprocess_v1"

# æˆªæ–­ç­–ç•¥ï¼šhead 2000 + tail 1000ï¼ˆChatGPT å»ºè®®ï¼šops æ–‡æ¡£å‘½ä»¤å¸¸åœ¨ååŠæ®µï¼‰
HEAD_CHARS = 2000
TAIL_CHARS = 1000

# å…¨å±€é€Ÿç‡æ§åˆ¶ï¼ˆGemini å»ºè®®ï¼šé˜² provider é™æµï¼‰
_rate_semaphore = Semaphore(MAX_WORKERS)


# â”€â”€ Evidence Flagsï¼ˆæ­£åˆ™ï¼Œé›¶ LLM æˆæœ¬ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compute_evidence_flags(content: str) -> dict:
    """ç”¨æ­£åˆ™æ£€æµ‹æ–‡æ¡£ä¸­çš„è¯æ®ç±»å‹ã€‚"""
    return {
        "has_command": bool(re.search(
            r'(```(?:bash|sh|shell|console)[\s\S]*?```'
            r'|^\s*\$\s+\w+'
            r'|`(?:redis-cli|kubectl|docker|curl|pip|npm|git|make)\b[^`]*`)',
            content, re.MULTILINE)),
        "has_config": bool(re.search(
            r'(```(?:yaml|yml|toml|ini|conf|json|xml|env|properties)[\s\S]*?```'
            r'|(?:^|\n)\s*\w+\s*[:=]\s*\S+)',
            content, re.MULTILINE)),
        "has_code_block": bool(re.search(r'```\w*\n', content)),
        "has_steps": bool(re.search(
            r'(^#{1,4}\s+(?:step\s+\d|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ­¥|\d+\.\s)'
            r'|^(?:\d+)\.\s+\*\*)',
            content, re.MULTILINE | re.IGNORECASE)),
    }


# â”€â”€ ç¡®å®šæ€§ Gap Flags èåˆï¼ˆChatGPT å»ºè®®ï¼šä¸å®Œå…¨ä¾èµ– LLMï¼‰â”€â”€â”€â”€

def merge_gap_flags(llm_gaps: list, doc_type: str,
                    evidence_flags: dict) -> list:
    """ç”¨ç¡®å®šæ€§è§„åˆ™èåˆ gap_flagsï¼ŒLLM è¾“å‡ºä»…ä½œè¡¥å……ã€‚"""
    gaps = set(llm_gaps) if llm_gaps else set()

    # è§„åˆ™ï¼šguide/tutorial/troubleshooting ç±»æ–‡æ¡£åº”è¯¥æœ‰å‘½ä»¤
    if doc_type in ("guide", "tutorial", "troubleshooting"):
        if not evidence_flags.get("has_command"):
            gaps.add("missing_command")
        if not evidence_flags.get("has_config"):
            gaps.add("missing_config")

    return sorted(gaps)


# â”€â”€ Content Hash â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def content_hash(text: str) -> str:
    return hashlib.sha1(text.encode()).hexdigest()


def load_cache() -> dict:
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE) as f:
            return json.load(f)
    return {}


def save_cache(cache: dict) -> None:
    os.makedirs(os.path.dirname(CACHE_FILE) or ".", exist_ok=True)
    with open(CACHE_FILE, "w") as f:
        json.dump(cache, f, indent=2)


# â”€â”€ æˆªæ–­ç­–ç•¥ï¼šhead + tail + code blocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def smart_truncate(content: str) -> str:
    """head 2000 + tail 1000ï¼Œä¸­é—´ç”¨çœç•¥æ ‡è®°ã€‚"""
    if len(content) <= HEAD_CHARS + TAIL_CHARS:
        return content
    head = content[:HEAD_CHARS]
    tail = content[-TAIL_CHARS:]
    return f"{head}\n\n[... ä¸­é—´å†…å®¹çœç•¥ ...]\n\n{tail}"


# â”€â”€ LLM è°ƒç”¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT = "You are a technical documentation analyst. Return only valid JSON."

USER_PROMPT_TEMPLATE = """Analyze this document and return a JSON object.

<document>
Title: {title}
Source: {source_repo} / {source_path}

{content}
</document>

Return ONLY a JSON object with these fields:
- "contextual_summary": One sentence (max 50 words) describing what this document covers. Start with the document subject, not "This document".
- "doc_type": One of: tutorial, reference, guide, troubleshooting, overview, example
- "quality_score": Integer 0-10. Score based on: actionability (has concrete steps/commands?), specificity (covers topic in depth?), structure (well-organized?).
- "key_concepts": Array of 3-5 key technical terms from this document.
- "gap_flags": Array of applicable flags from: "missing_command", "missing_config", "missing_example", "incomplete_steps". Empty array if no gaps.

JSON only, no markdown fences, no explanation."""


def _extract_json(raw: str) -> Optional[dict]:
    """ä» LLM è¾“å‡ºä¸­æå– JSONï¼ˆGemini å»ºè®®ï¼šæ­£åˆ™æ¸…æ´—ï¼‰ã€‚"""
    raw = raw.strip()
    # å»æ‰ markdown å›´æ 
    if raw.startswith("```"):
        raw = re.sub(r'^```\w*\n?', '', raw)
        raw = re.sub(r'\n?```$', '', raw)
    # æå–ç¬¬ä¸€ä¸ª {...} å—
    match = re.search(r'\{.*\}', raw, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            return None
    return None


def call_llm(title: str, source_repo: str, source_path: str,
             content: str) -> Optional[dict]:
    """è°ƒç”¨ LLM åˆ†æå•ä¸ªæ–‡æ¡£ã€‚"""
    from llm_client import get_client

    prompt = USER_PROMPT_TEMPLATE.format(
        title=title,
        source_repo=source_repo or "local",
        source_path=source_path or "",
        content=smart_truncate(content),
    )

    _rate_semaphore.acquire()
    try:
        client = get_client("doc_process")
        raw = client.generate(prompt, max_tokens=500, temperature=0,
                              system=SYSTEM_PROMPT)
        result = _extract_json(raw)
        if result is None:
            log.warning(f"  JSON è§£æå¤±è´¥: {raw[:200]}")
            return None

        # æ ¡éªŒå¿…éœ€å­—æ®µ
        required = ["contextual_summary", "doc_type", "quality_score",
                     "key_concepts", "gap_flags"]
        for field in required:
            if field not in result:
                log.warning(f"  ç¼ºå°‘å­—æ®µ {field}")
                return None

        # ç±»å‹æ ¡éªŒ + ä¿®æ­£
        valid_types = {"tutorial", "reference", "guide", "troubleshooting",
                       "overview", "example"}
        if result["doc_type"] not in valid_types:
            result["doc_type"] = "reference"
        result["quality_score"] = max(0, min(10, int(result["quality_score"])))

        return result
    except Exception as e:
        log.error(f"  LLM è°ƒç”¨å¤±è´¥: {e}")
        return None
    finally:
        _rate_semaphore.release()


# â”€â”€ å¤„ç†å•ä¸ªæ–‡æ¡£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _sidecar_path(filepath: str) -> str:
    p = Path(filepath)
    return str(p.parent / SIDECAR_DIR / (p.stem + ".json"))


def process_doc(filepath: str, force: bool = False,
                cache: Optional[dict] = None) -> Optional[str]:
    """å¤„ç†å•ä¸ªæ–‡æ¡£ã€‚è¿”å› sidecar è·¯å¾„ï¼Œè·³è¿‡è¿”å› Noneã€‚"""
    post = frontmatter.load(filepath)
    content = post.content
    h = content_hash(content)

    # å¢é‡è·³è¿‡
    if not force and cache and cache.get(filepath) == h:
        sidecar = _sidecar_path(filepath)
        if os.path.exists(sidecar):
            return None

    title = post.metadata.get("title", os.path.basename(filepath))
    source_repo = post.metadata.get("source_repo", "")
    source_path = post.metadata.get("source_path", "")

    # æ­£åˆ™ evidence_flagsï¼ˆå§‹ç»ˆè®¡ç®—ï¼‰
    evidence_flags = compute_evidence_flags(content)

    # LLM åˆ†æ
    llm_result = call_llm(title, source_repo, source_path, content)

    # ç»„è£… sidecarï¼ˆChatGPT å»ºè®®ï¼šLLM å¤±è´¥ä¹Ÿå†™ï¼Œè‡³å°‘ evidence_flags å¯ç”¨ï¼‰
    if llm_result:
        gap_flags = merge_gap_flags(
            llm_result.get("gap_flags", []),
            llm_result.get("doc_type", "reference"),
            evidence_flags,
        )
        sidecar_data = {
            "schema_version": SCHEMA_VERSION,
            "content_hash": f"sha1:{h}",
            "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "model": os.environ.get("DOC_PROCESS_MODEL", "unknown"),
            "prompt_version": PROMPT_VERSION,
            "llm_status": "ok",
            "contextual_summary": llm_result["contextual_summary"],
            "doc_type": llm_result["doc_type"],
            "quality_score": llm_result["quality_score"],
            "key_concepts": llm_result["key_concepts"],
            "gap_flags": gap_flags,
            "evidence_flags": evidence_flags,
        }
    else:
        # LLM å¤±è´¥ï¼šä»å†™ sidecarï¼Œevidence_flags ä»æœ‰ä»·å€¼
        sidecar_data = {
            "schema_version": SCHEMA_VERSION,
            "content_hash": f"sha1:{h}",
            "generated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "model": os.environ.get("DOC_PROCESS_MODEL", "unknown"),
            "prompt_version": PROMPT_VERSION,
            "llm_status": "failed",
            "contextual_summary": "",
            "doc_type": "",
            "quality_score": 0,
            "key_concepts": [],
            "gap_flags": [],
            "evidence_flags": evidence_flags,
        }

    # å†™ sidecar
    sidecar_path = _sidecar_path(filepath)
    os.makedirs(os.path.dirname(sidecar_path), exist_ok=True)
    with open(sidecar_path, "w") as f:
        json.dump(sidecar_data, f, ensure_ascii=False, indent=2)

    # æ›´æ–°ç¼“å­˜
    if cache is not None:
        cache[filepath] = h

    status = "âœ…" if llm_result else "âš ï¸ (evidence_flags only)"
    return f"{status} {sidecar_path}"


# â”€â”€ æ‰¹é‡å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_directory(docs_dir: str, force: bool = False) -> dict:
    md_files = sorted(Path(docs_dir).rglob("*.md"))
    # è·³è¿‡ .preprocess ç›®å½•ä¸‹çš„æ–‡ä»¶
    md_files = [f for f in md_files if SIDECAR_DIR not in f.parts]
    if not md_files:
        log.info(f"ç›®å½• {docs_dir} ä¸‹æ²¡æœ‰ .md æ–‡ä»¶")
        return {"total": 0, "processed": 0, "skipped": 0, "failed": 0}

    cache = load_cache()
    stats = {"total": len(md_files), "processed": 0, "skipped": 0,
             "failed": 0, "llm_failed": 0}

    log.info(f"ğŸ“¦ é¢„å¤„ç†: {len(md_files)} ä¸ªæ–‡ä»¶ ({docs_dir})")
    t0 = time.time()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {}
        for f in md_files:
            future = executor.submit(process_doc, str(f), force, cache)
            futures[future] = str(f)

        for future in as_completed(futures):
            filepath = futures[future]
            try:
                result = future.result()
                if result is None:
                    stats["skipped"] += 1
                elif "âš ï¸" in result:
                    stats["llm_failed"] += 1
                    stats["processed"] += 1
                    log.info(f"  {result}")
                else:
                    stats["processed"] += 1
                    log.info(f"  {result}")
            except Exception as e:
                stats["failed"] += 1
                log.error(f"  âŒ {filepath}: {e}")

    save_cache(cache)
    elapsed = time.time() - t0
    log.info(f"\nğŸ“Š é¢„å¤„ç†å®Œæˆ ({elapsed:.1f}s):")
    log.info(f"  å¤„ç†: {stats['processed']} | è·³è¿‡: {stats['skipped']} "
             f"| å¤±è´¥: {stats['failed']} | LLMå¤±è´¥: {stats['llm_failed']} "
             f"| å…±: {stats['total']}")
    return stats


def show_status(docs_dir: str) -> None:
    md_files = list(Path(docs_dir).rglob("*.md"))
    md_files = [f for f in md_files if SIDECAR_DIR not in f.parts]
    sidecars = list(Path(docs_dir).rglob(f"{SIDECAR_DIR}/*.json"))

    log.info(f"ç›®å½•: {docs_dir}")
    log.info(f"  æ–‡æ¡£æ•°: {len(md_files)}")
    log.info(f"  å·²é¢„å¤„ç†: {len(sidecars)}")
    log.info(f"  æœªå¤„ç†: {len(md_files) - len(sidecars)}")

    type_counts = {}
    quality_scores = []
    llm_ok = 0
    llm_fail = 0
    for s in sidecars:
        with open(s) as f:
            data = json.load(f)
        if data.get("llm_status") == "ok":
            llm_ok += 1
            dt = data.get("doc_type", "unknown")
            type_counts[dt] = type_counts.get(dt, 0) + 1
            quality_scores.append(data.get("quality_score", 0))
        else:
            llm_fail += 1

    if type_counts:
        log.info(f"  ç±»å‹åˆ†å¸ƒ: {type_counts}")
    if quality_scores:
        avg = sum(quality_scores) / len(quality_scores)
        log.info(f"  å¹³å‡è´¨é‡åˆ†: {avg:.1f}/10")
    if llm_fail:
        log.info(f"  LLM å¤±è´¥: {llm_fail} (ä»… evidence_flags)")


# â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main() -> None:
    parser = argparse.ArgumentParser(description="æ–‡æ¡£é¢„å¤„ç†å·¥å…·")
    parser.add_argument("--dir", nargs="+", help="å¤„ç†ç›®å½•ï¼ˆæ”¯æŒå¤šä¸ªï¼‰")
    parser.add_argument("--file", help="å¤„ç†å•ä¸ªæ–‡ä»¶")
    parser.add_argument("--status", metavar="DIR", help="æŸ¥çœ‹é¢„å¤„ç†çŠ¶æ€")
    parser.add_argument("--force", action="store_true", help="å¿½ç•¥ç¼“å­˜")
    args = parser.parse_args()

    if args.status:
        show_status(args.status)
    elif args.file:
        cache = load_cache()
        result = process_doc(args.file, args.force, cache)
        save_cache(cache)
        if result:
            log.info(result)
        else:
            log.info("â­ï¸  è·³è¿‡ï¼ˆæœªå˜æ›´ï¼‰")
    elif args.dir:
        for d in args.dir:
            process_directory(d, args.force)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
