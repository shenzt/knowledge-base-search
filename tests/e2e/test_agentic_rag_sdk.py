#!/usr/bin/env python3
"""Agentic RAG è‡ªåŠ¨åŒ–æµ‹è¯• v5 â€” å¤šæ•°æ®é›†æ”¯æŒ

æ•°æ®æº: redis-docs + awesome-llm-apps + local docs/ + RAGBench techqa + CRAG finance
è¯„ä¼°: eval_module (Gate é—¨ç¦ + RAGAS faithfulness/relevancy)

ç¯å¢ƒå˜é‡:
  EVAL_DATASET=v5|ragbench|crag|all  é€‰æ‹©æ•°æ®é›†ï¼ˆé»˜è®¤ v5ï¼‰
  EVAL_CONCURRENCY=N                 å¹¶å‘æ•°ï¼ˆé»˜è®¤ 1ï¼Œå»ºè®® 3-5ï¼‰
"""

import asyncio
import json
import os
import sys
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
load_dotenv()

from claude_agent_sdk import query, ClaudeAgentOptions

PROJECT_ROOT = Path(__file__).parent.parent.parent

# å¯¼å…¥è¯„æµ‹æ¨¡å—
sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
from eval_module import extract_contexts, gate_check, get_tools_used, get_retrieved_doc_paths, get_kb_commit, llm_judge, extract_turn_timings

# å¯¼å…¥æµ‹è¯•ç”¨ä¾‹
sys.path.insert(0, str(PROJECT_ROOT / "tests" / "fixtures"))
from v5_test_queries import TEST_CASES_V5, GOLDEN_CASES

# æ•°æ®é›†é€‰æ‹©
EVAL_DATASET = os.environ.get("EVAL_DATASET", "v5")

if EVAL_DATASET == "ragbench":
    from ragbench_techqa_cases import RAGBENCH_TECHQA_CASES
    TEST_CASES = RAGBENCH_TECHQA_CASES
elif EVAL_DATASET == "crag":
    from crag_finance_cases import CRAG_FINANCE_CASES
    TEST_CASES = CRAG_FINANCE_CASES
elif EVAL_DATASET == "all":
    from ragbench_techqa_cases import RAGBENCH_TECHQA_CASES
    from crag_finance_cases import CRAG_FINANCE_CASES
    TEST_CASES = TEST_CASES_V5 + RAGBENCH_TECHQA_CASES + CRAG_FINANCE_CASES
elif EVAL_DATASET == "golden":
    TEST_CASES = GOLDEN_CASES
    print(f"ğŸŒŸ Golden dataset: {len(TEST_CASES)} cases")
else:
    TEST_CASES = TEST_CASES_V5

# Smoke test è¿‡æ»¤ï¼šåªè·‘æŒ‡å®š ID çš„ç”¨ä¾‹
_SMOKE_IDS = os.environ.get("EVAL_SMOKE_IDS", "")
if _SMOKE_IDS:
    _ids = set(_SMOKE_IDS.split(","))
    TEST_CASES = [tc for tc in TEST_CASES if tc["id"] in _ids]
    print(f"ğŸ”¥ Smoke test: {len(TEST_CASES)} cases ({_SMOKE_IDS})")

# ä» v5 ç”¨ä¾‹çš„ expected_keywords åŠ¨æ€æ„å»º KEYWORD_CHECKS
KEYWORD_CHECKS = {}
for _tc in TEST_CASES:
    cat = _tc["category"]
    kws = _tc.get("expected_keywords", [])
    if cat not in KEYWORD_CHECKS:
        KEYWORD_CHECKS[cat] = list(kws)
    else:
        for kw in kws:
            if kw not in KEYWORD_CHECKS[cat]:
                KEYWORD_CHECKS[cat].append(kw)

# æ˜¯å¦å¯ç”¨ MCPï¼ˆæ¨¡å‹åŠ è½½éœ€è¦ 15-20 åˆ†é’Ÿï¼Œå¯é€‰å…³é—­ï¼‰
USE_MCP = os.environ.get("USE_MCP", "0") == "1"
# æ˜¯å¦å¯ç”¨ LLM-as-Judgeï¼ˆå¯¹ Gate é€šè¿‡çš„ç”¨ä¾‹åšè´¨é‡æ‰“åˆ†ï¼‰
USE_JUDGE = os.environ.get("USE_JUDGE", "0") == "1"
# å¹¶å‘æ•°ï¼ˆé»˜è®¤ 1 = ä¸²è¡Œï¼Œå»ºè®® 3-5ï¼‰
EVAL_CONCURRENCY = int(os.environ.get("EVAL_CONCURRENCY", "1"))
# Router æ¨¡å¼ï¼šé€šè¿‡ claude-code-router ä»£ç†åˆ°å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ GLM-5ï¼‰
# ç”¨æ³•: USE_ROUTER=1 ROUTER_MODEL=glm-5 ï¼ˆéœ€è¦å…ˆ ccr start å¯åŠ¨ routerï¼‰
# æ”¯æŒæ¨¡å‹: glm-5, qwen3.5-plus, deepseek-chat
USE_ROUTER = os.environ.get("USE_ROUTER", "0") == "1"
ROUTER_URL = os.environ.get("ROUTER_URL", "http://127.0.0.1:3456")
ROUTER_MODEL = os.environ.get("ROUTER_MODEL", "")  # ç©º=ä½¿ç”¨ router é»˜è®¤é…ç½®
# ç›´è¿æ¨¡å¼ï¼šAnthropic å…¼å®¹ APIï¼ˆå¦‚ MiniMax M2.5ï¼‰
# ç”¨æ³•: ANTHROPIC_BASE_URL=https://api.minimaxi.com/anthropic ANTHROPIC_API_KEY=sk-xxx MODEL_NAME=MiniMax-M2.5
# æ— éœ€ routerï¼Œç›´æ¥è®¾ç½® base_url + api_keyï¼ŒAgent SDK åŸç”Ÿæ”¯æŒ
DIRECT_BASE_URL = os.environ.get("ANTHROPIC_BASE_URL", "")
DIRECT_API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
MODEL_NAME = os.environ.get("MODEL_NAME", "")
USE_DIRECT = bool(DIRECT_BASE_URL and DIRECT_API_KEY)


def _switch_router_model(model_name: str):
    """åŠ¨æ€åˆ‡æ¢ router é»˜è®¤æ¨¡å‹ï¼ˆä¿®æ”¹ config.json + restartï¼‰ã€‚"""
    import json as _json
    config_path = os.path.expanduser("~/.claude-code-router/config.json")
    try:
        with open(config_path) as f:
            config = _json.load(f)
        # æ‰¾åˆ°åŒ…å«è¯¥æ¨¡å‹çš„ provider
        provider_name = None
        for p in config.get("Providers", []):
            if model_name in p.get("models", []):
                provider_name = p["name"]
                break
        if not provider_name:
            print(f"âš ï¸ æ¨¡å‹ {model_name} æœªåœ¨ router config ä¸­æ‰¾åˆ°ï¼Œè·³è¿‡åˆ‡æ¢")
            return
        route = f"{provider_name},{model_name}"
        config["Router"]["default"] = route
        config["Router"]["think"] = route
        with open(config_path, "w") as f:
            _json.dump(config, f, indent=2)
        # Restart router
        os.system("ccr restart > /dev/null 2>&1")
        import time; time.sleep(3)
        print(f"âœ… Router å·²åˆ‡æ¢åˆ° {route}")
    except Exception as e:
        print(f"âš ï¸ Router åˆ‡æ¢å¤±è´¥: {e}")


if USE_DIRECT:
    # ç›´è¿æ¨¡å¼ï¼šAnthropic å…¼å®¹ APIï¼ˆMiniMax ç­‰ï¼‰
    # Claude CLI ç”¨ ANTHROPIC_AUTH_TOKEN åš Authorization header
    os.environ["ANTHROPIC_AUTH_TOKEN"] = DIRECT_API_KEY
    os.environ.setdefault("DISABLE_COST_WARNINGS", "true")
    print(f"ğŸ”— ç›´è¿æ¨¡å¼: {DIRECT_BASE_URL} | model={MODEL_NAME or 'default'}")
elif USE_ROUTER:
    # è®¾ç½®ç¯å¢ƒå˜é‡è®© Agent SDK èµ° router ä»£ç†
    os.environ["ANTHROPIC_BASE_URL"] = ROUTER_URL
    os.environ["ANTHROPIC_AUTH_TOKEN"] = os.environ.get("ANTHROPIC_AUTH_TOKEN", "test")
    os.environ.setdefault("DISABLE_COST_WARNINGS", "true")
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼ŒåŠ¨æ€æ›´æ–° router config
    if ROUTER_MODEL:
        _switch_router_model(ROUTER_MODEL)

if USE_MCP:
    # ä¸ä½¿ç”¨ setting_sources=["project"]ï¼Œå› ä¸ºå®ƒä¼šåŠ è½½ .mcp.json å¹¶è¦†ç›– allowed_tools
    # æ”¹ç”¨ system_prompt æ³¨å…¥å…³é”®æŒ‡ä»¤ + æ‰‹åŠ¨é…ç½® MCP server
    SEARCH_SYSTEM_PROMPT = """ä½ æ˜¯çŸ¥è¯†åº“æ£€ç´¢åŠ©æ‰‹ã€‚æ”¶åˆ°é—®é¢˜åï¼Œç”¨å·¥å…·æ£€ç´¢æ–‡æ¡£ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹å›ç­”ã€‚

## çŸ¥è¯†åº“ç›®å½•ï¼ˆæ‰€æœ‰æ–‡æ¡£éƒ½åœ¨è¿™é‡Œï¼‰

1. Redis å®˜æ–¹æ–‡æ¡£ï¼ˆ294 docsï¼Œåœ¨ Qdrant ç´¢å¼•ä¸­ï¼‰ï¼š
   - develop/: data-types/, programmability/, reference/, pubsub/, get-started/
   - operate/: management/(sentinel, security, optimization), install/, reference/
   è·¯å¾„ç¤ºä¾‹: kb/kb-redis-docs/docs/redis-docs/develop/data-types/streams.md

2. awesome-llm-appsï¼ˆ207 docsï¼Œåœ¨ Qdrant ç´¢å¼•ä¸­ï¼‰ï¼š
   - rag_tutorials/, advanced_ai_agents/, starter_ai_agents/, ai_agent_framework_crash_course/
   è·¯å¾„ç¤ºä¾‹: kb/kb-awesome-llm-apps/docs/awesome-llm-apps/rag_tutorials/xxx/README.md

3. æœ¬åœ°æ–‡æ¡£ï¼ˆGrep å¯æœç´¢ï¼‰ï¼š
   - docs/runbook/    â€” è¿ç»´ runbookï¼ˆRedis æ•…éšœæ¢å¤ã€K8s æ’éšœï¼‰
   - docs/api/        â€” API è®¾è®¡æ–‡æ¡£ï¼ˆè®¤è¯ã€æˆæƒï¼‰

4. RAGBench techqaï¼ˆ245 docsï¼‰ã€CRAG financeï¼ˆ119 docsï¼‰â€” åœ¨ Qdrant ç´¢å¼•ä¸­

## æœ¬åœ°æ–‡ä»¶æœç´¢èŒƒå›´ï¼ˆGrep/Glob/Readï¼‰

çŸ¥è¯†åº“æ–‡ä»¶åªåœ¨ä»¥ä¸‹ç›®å½•ï¼š
- docs/runbook/    â€” è¿ç»´ runbookï¼ˆRedis æ•…éšœæ¢å¤ã€K8s æ’éšœï¼‰
- docs/api/        â€” API è®¾è®¡æ–‡æ¡£ï¼ˆè®¤è¯ã€æˆæƒï¼‰
- kb/kb-redis-docs/docs/  â€” Qdrant ç´¢å¼•çš„å®Œæ•´æ–‡æ¡£ï¼ˆhybrid_search è¿”å›çš„ pathï¼‰

æ­£ç¡®: Grep(pattern="sentinel", path="docs/runbook/")
æ­£ç¡®: Grep(pattern="OAuth", path="docs/api/")
æ­£ç¡®: Read(file_path="kb/kb-redis-docs/docs/redis-docs/operate/...")  â† æ¥è‡ª hybrid_search è¿”å›çš„ path
é”™è¯¯: Grep(pattern="sentinel", path="docs/")  â† ä¼šæœåˆ° ragbench/crag å™ªå£°
é”™è¯¯: Grep(pattern="sentinel", path=".")  â† ä¼šæœåˆ° tests/eval/scripts
é”™è¯¯: Read(file_path="docs/ragbench-techqa/...")  â† è¯„æµ‹æ•°æ®ï¼Œä¸æ˜¯çŸ¥è¯†åº“
é”™è¯¯: Read(file_path="tests/fixtures/...")  â† æµ‹è¯•ä»£ç ï¼Œä¸æ˜¯çŸ¥è¯†åº“

## æ£€ç´¢æ–¹æ³•ï¼ˆç¬¬ä¸€æ­¥ï¼Œå¿…é¡»æ‰§è¡Œï¼‰

æ¯æ¬¡æ”¶åˆ°é—®é¢˜ï¼Œç«‹å³å¹¶è¡Œè°ƒç”¨è¿™ä¸¤ä¸ªå·¥å…·ï¼š

  mcp__knowledge-base__hybrid_search(query="<é—®é¢˜å…³é”®è¯>", top_k=5)
  Grep(pattern="<å…³é”®è¯>", path="docs/runbook/")

hybrid_search æœç´¢ Qdrant ç´¢å¼•ï¼ˆRedis æ–‡æ¡£ã€LLM æ–‡æ¡£ç­‰å…¨éƒ¨åœ¨è¿™é‡Œï¼‰ã€‚
Grep æœç´¢æœ¬åœ° docs/ å­ç›®å½•ï¼ˆä»… runbook/api ä¸¤ä¸ªç›®å½•ï¼‰ã€‚

Grep path é€‰æ‹©ï¼š
- Redis/K8s â†’ path="docs/runbook/"
- API/OAuth â†’ path="docs/api/"

## æ‰©å±•é˜…è¯»ï¼ˆç¬¬äºŒæ­¥ï¼ŒæŒ‰éœ€æ‰§è¡Œï¼‰

hybrid_search è¿”å›çš„ path å­—æ®µæ˜¯æ–‡æ¡£çš„å®é™…è·¯å¾„ï¼Œç”¨ Read(file_path=path) è¯»å–å®Œæ•´å†…å®¹ã€‚
å¦‚æœ chunk åªæœ‰æ¦‚è¿°ç¼ºå°‘ç»†èŠ‚ï¼Œå¿…é¡» Read å®Œæ•´æ–‡æ¡£ã€‚

## æ£€ç´¢æ•ˆç‡

1. **è§å¥½å°±æ”¶**ï¼šhybrid_search çš„ top-1 title/path ä¸é—®é¢˜ç›´æ¥ç›¸å…³ â†’ Read åï¼Œå¦‚æœèƒ½å¼•ç”¨åˆ°å›ç­”é—®é¢˜çš„å…³é”®å¥ï¼ˆå®šä¹‰/æ­¥éª¤/å‘½ä»¤/é…ç½®ï¼‰â†’ ç›´æ¥å›ç­”ã€‚å¦‚æœ Read ååªæœ‰æ¦‚å¿µæ²¡æœ‰ç»†èŠ‚ï¼Œä¸”é—®é¢˜æ˜¯"æ€ä¹ˆåš/é…ç½®/æ’éšœ" â†’ å†æœä¸€æ¬¡è¡¥è¯æ®ï¼Œæˆ–å£°æ˜"æ–‡æ¡£æœªæä¾›å…·ä½“æ­¥éª¤"
2. **å¿«é€Ÿæ”¾å¼ƒ**ï¼šç¬¬ä¸€æ¬¡ hybrid_search æ— ç›¸å…³ç»“æœï¼ˆtop-5 çš„ title/path éƒ½ä¸å«é—®é¢˜æ ¸å¿ƒè¯ï¼‰â†’ æ”¹å†™ query å†æœä¸€æ¬¡ï¼ˆæ¢è¯­è¨€/æå–æ ¸å¿ƒåè¯ï¼‰ã€‚ç¬¬äºŒæ¬¡ top-5 title/path ä»ä¸å«æ ¸å¿ƒè¯ â†’ å›ç­”"âŒ æœªæ‰¾åˆ°"
3. **ç¦æ­¢å¾ªç¯**ï¼šè¿ç»­ Grep/Glob 2 æ¬¡æœªå‘½ä¸­ â†’ å¿…é¡»æ¢åˆ° hybrid_search/keyword_searchï¼Œæˆ–ç›´æ¥å›ç­”/notfoundã€‚ç¦æ­¢ç»§ç»­æ¢ pattern å †å 

## å›ç­”è§„åˆ™

- 100% åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œé™„å¼•ç”¨ [æ¥æº: path]
- ä¸¥ç¦ç”¨è®­ç»ƒçŸ¥è¯†è¡¥å……å‘½ä»¤/é…ç½®/ä»£ç 
- æ–‡æ¡£æ— ç›¸å…³ä¿¡æ¯ â†’ å›ç­”"âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
- å›ç­”è¯­è¨€è·ŸéšæŸ¥è¯¢è¯­è¨€

## ç¦æ­¢

- ç¦æ­¢ Grep(path="docs/") â€” ä¼šå‘½ä¸­ ragbench/crag å™ªå£°æ–‡ä»¶
- ç¦æ­¢ Grep(path=".") æˆ– Grep ä¸å¸¦ path â€” ä¼šæœåˆ° tests/eval/scripts
- ç¦æ­¢ Grep æ‰«æ docs/ragbench-techqa/ã€docs/crag-finance/ã€eval/ã€tests/ã€scripts/
- ç¦æ­¢ Read tests/fixtures/kb-sources/ ä¸‹çš„æ–‡ä»¶ â€” é‚£æ˜¯æµ‹è¯• fixturesï¼Œä¸æ˜¯çŸ¥è¯†åº“
- ç¦æ­¢çŒœæµ‹æ–‡ä»¶è·¯å¾„ â€” Read çš„è·¯å¾„å¿…é¡»æ¥è‡ªå·¥å…·è¿”å›å€¼
- ç¦æ­¢åªç”¨ Grep ä¸ç”¨ hybrid_search â€” Redis æ–‡æ¡£ä¸åœ¨æœ¬åœ° docs/ ä¸‹ï¼Œåªæœ‰ hybrid_search èƒ½æ‰¾åˆ°
"""
    BASE_OPTIONS = dict(
        allowed_tools=[
            "Read", "Grep", "Glob",
            "mcp__knowledge-base__hybrid_search",
            "mcp__knowledge-base__keyword_search",
            "mcp__knowledge-base__index_status",
        ],
        disallowed_tools=["Bash", "Write", "Edit", "NotebookEdit", "Task"],
        mcp_servers={
            "knowledge-base": {
                "command": sys.executable,
                "args": [str(PROJECT_ROOT / "scripts" / "mcp_server.py")],
                "env": {
                    "QDRANT_URL": os.environ.get("QDRANT_URL", "http://localhost:6333"),
                    "COLLECTION_NAME": os.environ.get("COLLECTION_NAME", "knowledge-base"),
                },
            }
        },
        system_prompt=SEARCH_SYSTEM_PROMPT,
        permission_mode="bypassPermissions",
        cwd=str(PROJECT_ROOT),
        max_turns=10,
        **({"model": MODEL_NAME} if MODEL_NAME else {}),
    )
else:
    # æ—  MCP æ¨¡å¼ï¼šä»…ä½¿ç”¨ Grep/Glob/Readï¼ˆAgentic Layer 1ï¼‰
    # æ³¨æ„ï¼šä¸èƒ½ç”¨ setting_sources=["project"]ï¼Œå¦åˆ™ä¼šåŠ è½½ .mcp.json å¯åŠ¨ MCP server
    # æ”¹ç”¨ system_prompt æ³¨å…¥ CLAUDE.md å’Œ search skill çš„å…³é”®æŒ‡ä»¤
    BASE_OPTIONS = dict(
        allowed_tools=["Read", "Grep", "Glob"],
        disallowed_tools=["Bash", "Write", "Edit", "NotebookEdit", "Task"],
        permission_mode="bypassPermissions",
        cwd=str(PROJECT_ROOT),
        max_turns=10,
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“æ£€ç´¢åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šç”¨ /search å‘½ä»¤æŸ¥è¯¢çŸ¥è¯†åº“ã€‚

çŸ¥è¯†åº“æ–‡æ¡£åœ¨ docs/ ç›®å½•ä¸‹ï¼Œæ ¼å¼ä¸º Markdownï¼ŒåŒ…å« Redisã€LLM/AI åº”ç”¨å¼€å‘ç­‰æŠ€æœ¯æ–‡æ¡£ã€‚
Qdrant å‘é‡ç´¢å¼•åŒ…å« redis-docsï¼ˆ234 docsï¼‰å’Œ awesome-llm-appsï¼ˆ207 docsï¼‰ã€‚

æ ¸å¿ƒæµç¨‹ï¼šæœç´¢ â†’ è¯„ä¼° â†’ æ‰©å±• â†’ å›ç­”

1. åˆå§‹æ£€ç´¢ï¼šä½¿ç”¨ Grep æœç´¢å…³é”®è¯ + hybrid_search è¯­ä¹‰æ£€ç´¢ï¼ˆå¯å¹¶è¡Œï¼‰
2. è¯„ä¼° chunk å……åˆ†æ€§ï¼šchunk æ˜¯å¦åŒ…å«å…·ä½“æ­¥éª¤/å‘½ä»¤/é…ç½®/ä»£ç ï¼Ÿè¿˜æ˜¯åªæœ‰æ¦‚è¿°ï¼Ÿ
3. æ‰©å±•ä¸Šä¸‹æ–‡ï¼šå¦‚æœ chunk ä¸å……åˆ†ï¼Œç”¨ Read(path) è¯»å– hybrid_search è¿”å›çš„ path å­—æ®µæŒ‡å‘çš„å®Œæ•´æ–‡ä»¶
4. åŸºäºè¯æ®å›ç­”ï¼Œå¿…é¡»å¸¦å¼•ç”¨ [æ¥æº: docs/xxx.md]

âš ï¸ Hard Groundingï¼ˆæœ€é‡è¦çš„è§„åˆ™ï¼‰ï¼š
- ä½ çš„å›ç­”å¿…é¡» 100% åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œé€å­—é€å¥æœ‰æ®å¯æŸ¥
- ä¸¥ç¦ç”¨ä½ è‡ªå·±çš„è®­ç»ƒçŸ¥è¯†è¡¥å……å‘½ä»¤ã€é…ç½®ã€ä»£ç ç¤ºä¾‹ã€å‚æ•°è¯´æ˜ã€æœ€ä½³å®è·µ
- å¦‚æœ chunk åªæœ‰æ¦‚è¿°çº§åˆ«å†…å®¹ï¼Œå¿…é¡» Read(path) è·å–å®Œæ•´æ–‡æ¡£å†å›ç­”
- å¦‚æœ Read åæ–‡æ¡£ä»ç„¶åªæœ‰æ¦‚å¿µæè¿°æ²¡æœ‰å…·ä½“å‘½ä»¤/é…ç½®/ä»£ç ï¼Œä½ å¿…é¡»ï¼š
  1. å›ç­”æ–‡æ¡£ä¸­å®é™…åŒ…å«çš„å†…å®¹
  2. æ˜ç¡®å£°æ˜ï¼š"âš ï¸ æ–‡æ¡£ä¸­æåˆ°äº† [æ¦‚å¿µ]ï¼Œä½†æœªæä¾›å…·ä½“çš„ [å‘½ä»¤/é…ç½®/ä»£ç ç¤ºä¾‹]ã€‚å¦‚éœ€è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚"
  3. ç»å¯¹ä¸è¦è‡ªå·±è¡¥å……ç¼ºå¤±çš„å‘½ä»¤/é…ç½®/ä»£ç 
- å¦‚æœæ–‡æ¡£ä¸­å®Œå…¨æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œåªå›ç­”"âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
- å®å¯å›ç­”ä¸å®Œæ•´ï¼Œä¹Ÿä¸è¦ç¼–é€ ä»»ä½•ç»†èŠ‚
- å›ç­”è¯­è¨€è·ŸéšæŸ¥è¯¢è¯­è¨€ï¼ˆä¸­æ–‡é—®ä¸­æ–‡ç­”ï¼Œè‹±æ–‡é—®è‹±æ–‡ç­”ï¼‰
- å¼•ç”¨å…·ä½“æ–‡æ¡£è·¯å¾„
""",
    )


def log(msg: str, log_file=None):
    """åŒæ—¶è¾“å‡ºåˆ° stdout å’Œæ—¥å¿—æ–‡ä»¶"""
    print(msg, flush=True)
    if log_file:
        log_file.write(msg + "\n")
        log_file.flush()


async def run_query(prompt: str, session_id: Optional[str], log_file) -> Dict[str, Any]:
    """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢ï¼Œå®Œæ•´è®°å½•ä¸­é—´è¿‡ç¨‹"""
    start = time.time()
    answer = ""
    new_session_id = session_id
    cost = 0.0
    num_turns = 0
    tools_used = []
    messages_log = []  # å®Œæ•´æ¶ˆæ¯æ—¥å¿—

    try:
        if session_id:
            opts = ClaudeAgentOptions(
                resume=session_id,
                permission_mode="bypassPermissions",
                max_turns=10,
            )
        else:
            opts = ClaudeAgentOptions(**BASE_OPTIONS)

        async for msg in query(prompt=prompt, options=opts):
            # è®°å½•æ¯æ¡æ¶ˆæ¯çš„å®Œæ•´ä¿¡æ¯
            msg_dict = {}
            for attr in ["type", "subtype", "role", "result", "session_id",
                         "cost_usd", "num_turns", "tool_name", "tool_input",
                         "tool_result", "content", "text", "data",
                         "stop_reason", "duration_ms", "total_cost_usd",
                         "usage", "modelUsage", "permission_denials"]:
                val = getattr(msg, attr, None)
                if val is not None:
                    try:
                        json.dumps(val)  # ç¡®ä¿å¯åºåˆ—åŒ–
                        msg_dict[attr] = val
                    except (TypeError, ValueError):
                        msg_dict[attr] = str(val)

            messages_log.append(msg_dict)

            # è¯¦ç»†æ—¥å¿—è¾“å‡º - è§£æ SDK æ¶ˆæ¯ç»“æ„
            subtype = getattr(msg, "subtype", None)
            content = getattr(msg, "content", None)

            if subtype == "init":
                data = getattr(msg, "data", {}) or {}
                if isinstance(data, dict):
                    new_session_id = data.get("session_id", new_session_id)
                    model = data.get("model", "unknown")
                    log(f"    [INIT] session={new_session_id} model={model}", log_file)
                if hasattr(msg, "session_id"):
                    new_session_id = msg.session_id

            elif subtype == "success":
                # æœ€ç»ˆç»“æœ
                if hasattr(msg, "result"):
                    answer = msg.result if isinstance(msg.result, str) else str(msg.result)
                num_turns = getattr(msg, "num_turns", num_turns)
                cost = getattr(msg, "total_cost_usd", cost) or cost
                duration = getattr(msg, "duration_ms", 0)
                log(f"    [RESULT] {len(answer)} chars | {num_turns} turns | {duration}ms", log_file)

            elif content:
                # è§£æ content blocksï¼ˆTextBlock, ToolUseBlock, ToolResultBlockï¼‰
                if isinstance(content, list):
                    for block in content:
                        block_type = getattr(block, "type", None)
                        if block_type is None and isinstance(block, dict):
                            block_type = block.get("type", "")

                        if block_type == "text" or (hasattr(block, "text") and not hasattr(block, "name") and not hasattr(block, "tool_use_id")):
                            # TextBlock - Claude çš„æ¨ç†/å›å¤
                            text = getattr(block, "text", "") if hasattr(block, "text") else block.get("text", "")
                            if text:
                                # å¤šè¡Œæ–‡æœ¬æˆªæ–­æ˜¾ç¤º
                                lines = text.strip().split("\n")
                                preview = lines[0][:200]
                                if len(lines) > 1:
                                    preview += f" (+{len(lines)-1} lines)"
                                log(f"    [THINK] {preview}", log_file)

                        elif block_type == "tool_use" or hasattr(block, "name"):
                            # ToolUseBlock - å·¥å…·è°ƒç”¨
                            tool_name = getattr(block, "name", "unknown")
                            tool_input = getattr(block, "input", {})
                            if isinstance(tool_input, dict):
                                input_str = json.dumps(tool_input, ensure_ascii=False)
                            else:
                                input_str = str(tool_input)
                            if len(input_str) > 300:
                                input_str = input_str[:300] + "..."
                            log(f"    [TOOL] {tool_name}({input_str})", log_file)
                            tools_used.append(tool_name)

                        elif block_type == "tool_result" or hasattr(block, "tool_use_id"):
                            # ToolResultBlock - å·¥å…·è¿”å›
                            result_content = getattr(block, "content", "")
                            if isinstance(result_content, str):
                                # æˆªæ–­é•¿ç»“æœï¼Œä¿ç•™å…³é”®ä¿¡æ¯
                                lines = result_content.strip().split("\n")
                                if len(lines) > 5:
                                    preview = "\n".join(lines[:3]) + f"\n    ... ({len(lines)} lines total)"
                                else:
                                    preview = result_content[:400]
                                log(f"    [TOOL_OUT] {preview}", log_file)
                            elif isinstance(result_content, list):
                                for rc in result_content:
                                    rc_text = getattr(rc, "text", str(rc)) if hasattr(rc, "text") else str(rc)
                                    log(f"    [TOOL_OUT] {rc_text[:300]}", log_file)

            # æå–å…ƒæ•°æ®
            if hasattr(msg, "cost_usd") and msg.cost_usd:
                cost = msg.cost_usd
            if hasattr(msg, "total_cost_usd") and msg.total_cost_usd:
                cost = msg.total_cost_usd
            if hasattr(msg, "num_turns") and msg.num_turns:
                num_turns = msg.num_turns

            # æ£€æŸ¥ permission denials
            denials = getattr(msg, "permission_denials", None)
            if denials:
                for d in denials:
                    tool = d.get("tool_name", "unknown") if isinstance(d, dict) else str(d)
                    log(f"    [DENIED] {tool}", log_file)

        elapsed = time.time() - start
        return {
            "status": "success",
            "answer": answer,
            "session_id": new_session_id,
            "elapsed": elapsed,
            "cost_usd": cost,
            "num_turns": num_turns,
            "tools_used": tools_used,
            "messages_log": messages_log,
        }
    except Exception as e:
        elapsed = time.time() - start
        log(f"    [ERROR] {e}", log_file)
        return {
            "status": "error",
            "error": str(e),
            "session_id": new_session_id,
            "elapsed": elapsed,
            "messages_log": messages_log,
        }


def evaluate(tc: Dict, result: Dict) -> Dict:
    """ä¸¤é˜¶æ®µè¯„ä¼°: Gate é—¨ç¦ (ç¡®å®šæ€§) â†’ å…³é”®è¯è´¨é‡æ£€æŸ¥ (è¾…åŠ©)ã€‚"""
    ev = {"passed": False, "reasons": [], "quality": {}, "gate": {}}

    if result["status"] != "success":
        ev["reasons"].append(f"æ‰§è¡Œå¤±è´¥: {result.get('error', '')[:80]}")
        return ev

    answer = result.get("answer", "")
    messages_log = result.get("messages_log", [])

    # â”€â”€ Stage 1: ç»“æ„åŒ– context æå– â”€â”€
    contexts = extract_contexts(messages_log)
    ev["quality"]["contexts_count"] = len(contexts)
    ev["quality"]["tools_used"] = get_tools_used(contexts)
    ev["quality"]["retrieved_paths"] = get_retrieved_doc_paths(contexts)

    # â”€â”€ Stage 1.5: Per-turn timing â”€â”€
    turn_timings = extract_turn_timings(messages_log)
    ev["quality"]["turn_timings"] = turn_timings

    # â”€â”€ Stage 2: Gate é—¨ç¦ â”€â”€
    gate = gate_check(tc, answer, contexts)
    ev["gate"] = gate

    if not gate["passed"]:
        ev["passed"] = False
        ev["reasons"] = gate["reasons"]
        return ev

    # â”€â”€ Stage 3: è´¨é‡æ£€æŸ¥ (Gate é€šè¿‡åçš„è¾…åŠ©æŒ‡æ ‡) â”€â”€
    if len(answer) < 50:
        ev["reasons"].append(f"ç­”æ¡ˆè¿‡çŸ­ ({len(answer)})")
        return ev

    # å¼•ç”¨è´¨é‡ (ä» gate è·å–)
    ev["quality"]["has_citation"] = gate["checks"].get("has_citation", False)

    # å…³é”®è¯åŒ¹é… (è¾…åŠ©ä¿¡å·ï¼Œä¸ä½œä¸º pass/fail åˆ¤æ®)
    expected = KEYWORD_CHECKS.get(tc["category"], [])
    matched = [k for k in expected if k.lower() in answer.lower()]
    ev["quality"]["keywords"] = matched

    # æ­£ç¡®æ–‡æ¡£å¼•ç”¨ (ä» gate çš„ expected_doc_hit è·å–)
    ev["quality"]["correct_doc"] = gate["checks"].get("expected_doc_hit", None)

    # Gate é€šè¿‡ + ç­”æ¡ˆè¶³å¤Ÿé•¿ â†’ é€šè¿‡
    if len(answer) >= 50:
        ev["passed"] = True
    else:
        ev["reasons"].append(f"ç­”æ¡ˆè¿‡çŸ­ ({len(answer)})")
        return ev

    # â”€â”€ Stage 4: LLM-as-Judge (å¯é€‰ï¼ŒGate é€šè¿‡å) â”€â”€
    if USE_JUDGE and ev["passed"] and not tc.get("source") == "notfound":
        gold_answer = tc.get("reference_answer")
        judge = llm_judge(tc["query"], answer, contexts, gold_answer=gold_answer)
        ev["quality"]["judge"] = judge
        # Judge score < 2 è§†ä¸ºè´¨é‡ä¸åˆæ ¼ï¼ˆä½†ä¸æ”¹å˜ pass/failï¼‰
        if judge.get("score", -1) >= 0:
            ev["quality"]["judge_score"] = judge["score"]
            ev["quality"]["faithfulness"] = judge.get("faithfulness", -1)
            ev["quality"]["relevancy"] = judge.get("relevancy", -1)
            ev["quality"]["context_precision"] = judge.get("context_precision", -1)
            ev["quality"]["context_recall"] = judge.get("context_recall", -1)
            ev["quality"]["answer_correctness"] = judge.get("answer_correctness", -1)

    return ev


async def run_single_case(i: int, tc: Dict, sem: asyncio.Semaphore,
                          results_slot: list, log_lock: threading.Lock,
                          lf, df) -> None:
    """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆå¹¶å‘å®‰å…¨ï¼‰ã€‚"""
    async with sem:
        case_log = []  # ç¼“å†²æ—¥å¿—ï¼Œå®Œæˆåä¸€æ¬¡æ€§å†™å…¥

        def clog(msg):
            case_log.append(msg)

        clog(f"\n{'='*60}")
        clog(f"[{i}/{len(TEST_CASES)}] {tc['id']} ({tc['category']}) [{tc.get('type', '?')}]")
        clog(f"  Q: {tc['query']}")
        if tc.get("note"):
            clog(f"  ğŸ’¡ {tc['note']}")
        clog(f"  å¼€å§‹: {datetime.now().strftime('%H:%M:%S')}")

        # æ¯ä¸ªç”¨ä¾‹ç‹¬ç«‹ sessionï¼Œä¸å¤ç”¨
        prompt = f"è¯·æ£€ç´¢çŸ¥è¯†åº“å¹¶å›ç­”: {tc['query']}"

        # run_query å†…éƒ¨çš„å®æ—¶æ—¥å¿—ç”¨ Noneï¼ˆä¸å†™æ–‡ä»¶ï¼‰ï¼Œé  case_log ç¼“å†²
        import io
        buf = io.StringIO()

        class BufWriter:
            def write(self, s): buf.write(s)
            def flush(self): pass

        # å•æ¬¡ call è¶…æ—¶ä¿æŠ¤ï¼ˆé˜²æ­¢ API hang ä½ï¼Œå¦‚ Kimi K2.5 case 16ï¼‰
        QUERY_TIMEOUT = int(os.environ.get("EVAL_QUERY_TIMEOUT", "300"))  # é»˜è®¤ 5 åˆ†é’Ÿ
        try:
            result = await asyncio.wait_for(
                run_query(prompt, None, BufWriter()),
                timeout=QUERY_TIMEOUT,
            )
        except asyncio.TimeoutError:
            result = {
                "status": "error",
                "error": f"TimeoutError: query exceeded {QUERY_TIMEOUT}s",
                "answer": "",
                "session_id": None,
                "cost_usd": 0,
                "num_turns": 0,
                "elapsed": QUERY_TIMEOUT,
                "messages": [],
            }
            clog(f"  â° è¶…æ—¶: query è¶…è¿‡ {QUERY_TIMEOUT}sï¼Œè·³è¿‡")

        # è¿½åŠ  run_query çš„è¯¦ç»†æ—¥å¿—
        detail_lines = buf.getvalue()
        if detail_lines:
            case_log.append(detail_lines.rstrip())

        elapsed = result.get("elapsed", 0)
        ev = evaluate(tc, result)

        if result["status"] == "error":
            clog(f"  âŒ é”™è¯¯: {result.get('error', '')[:80]}")
            status = "error"
        elif ev["passed"]:
            ans_len = len(result.get("answer", ""))
            quality = ev.get("quality", {})
            tools = quality.get("tools_used", [])
            cite = "å¼•ç”¨âœ…" if quality.get("has_citation") else "å¼•ç”¨âŒ"
            correct_doc = quality.get("correct_doc")
            doc_tag = "æ–‡æ¡£âœ…" if correct_doc else ("æ–‡æ¡£âŒ" if correct_doc is False else "")
            ctx_count = quality.get("contexts_count", 0)
            kw = quality.get("keywords", [])
            clog(f"  âœ… é€šè¿‡ | {ans_len}å­—ç¬¦ | {elapsed:.1f}s | ${result.get('cost_usd', 0):.4f} | {cite} {doc_tag} | ctx:{ctx_count}")
            if tools:
                clog(f"  ğŸ”§ å·¥å…·: {', '.join(tools)}")
            if quality.get("retrieved_paths"):
                clog(f"  ğŸ“„ æ£€ç´¢: {', '.join(quality['retrieved_paths'][:5])}")
            if kw:
                clog(f"  ğŸ”‘ å…³é”®è¯: {', '.join(kw)}")
            if quality.get("judge_score") is not None:
                js = quality["judge_score"]
                ff = quality.get("faithfulness", "?")
                rr = quality.get("relevancy", "?")
                clog(f"  ğŸ§‘â€âš–ï¸ Judge: score={js} faith={ff} rel={rr}")
            status = "passed"
        else:
            clog(f"  âŒ å¤±è´¥: {'; '.join(ev['reasons'])}")
            status = "failed"

        ans_preview = result.get("answer", "")[:500]
        if ans_preview:
            clog(f"  ğŸ“ ç­”æ¡ˆ: {ans_preview}{'...' if len(result.get('answer',''))>500 else ''}")
        clog(f"  ç»“æŸ: {datetime.now().strftime('%H:%M:%S')} | è€—æ—¶: {elapsed:.1f}s")
        clog("-" * 80)

        # æ„å»ºç»“æœ
        gate = ev.get("gate", {})
        quality = ev.get("quality", {})
        detail_record = {
            "test_id": tc["id"],
            "category": tc["category"],
            "type": tc.get("type", "unknown"),
            "source": tc.get("source", "unknown"),
            "query": tc["query"],
            "status": status,
            "elapsed_seconds": elapsed,
            "cost_usd": result.get("cost_usd", 0),
            "num_turns": result.get("num_turns", 0),
            "answer_length": len(result.get("answer", "")),
            "answer": result.get("answer", ""),
            "tools_used": quality.get("tools_used", []),
            "retrieved_paths": quality.get("retrieved_paths", []),
            "contexts_count": quality.get("contexts_count", 0),
            "has_citation": quality.get("has_citation", False),
            "correct_doc": quality.get("correct_doc"),
            "matched_keywords": quality.get("keywords", []),
            "gate_passed": gate.get("passed"),
            "gate_checks": gate.get("checks", {}),
            "failure_reasons": ev.get("reasons", []),
            "judge_score": quality.get("judge_score"),
            "faithfulness": quality.get("faithfulness"),
            "relevancy": quality.get("relevancy"),
            "context_precision": quality.get("context_precision"),
            "context_recall": quality.get("context_recall"),
            "answer_correctness": quality.get("answer_correctness"),
            "judge": quality.get("judge"),
            "turn_timings": quality.get("turn_timings", []),
            "messages": result.get("messages_log", []),
        }

        summary_record = {
            "test_id": tc["id"], "category": tc["category"],
            "type": tc.get("type", "unknown"),
            "source": tc.get("source", "unknown"),
            "query": tc["query"],
            "status": status, "elapsed_seconds": elapsed,
            "cost_usd": result.get("cost_usd", 0),
            "num_turns": result.get("num_turns", 0),
            "answer_length": len(result.get("answer", "")),
            "tools_used": quality.get("tools_used", []),
            "retrieved_paths": quality.get("retrieved_paths", []),
            "contexts_count": quality.get("contexts_count", 0),
            "has_citation": quality.get("has_citation", False),
            "correct_doc": quality.get("correct_doc"),
            "matched_keywords": quality.get("keywords", []),
            "gate_passed": gate.get("passed"),
            "failure_reasons": ev.get("reasons", []),
            "answer_preview": result.get("answer", "")[:300],
            "judge_score": quality.get("judge_score"),
            "faithfulness": quality.get("faithfulness"),
            "relevancy": quality.get("relevancy"),
            "context_precision": quality.get("context_precision"),
            "context_recall": quality.get("context_recall"),
            "answer_correctness": quality.get("answer_correctness"),
            "turn_timings": quality.get("turn_timings", []),
        }

        # çº¿ç¨‹å®‰å…¨å†™å…¥æ—¥å¿—å’Œç»“æœ
        with log_lock:
            for line in case_log:
                log(line, lf)
            df.write(json.dumps(detail_record, ensure_ascii=False) + "\n")
            df.flush()

        # å­˜å…¥å¯¹åº” slotï¼ˆä¿æŒé¡ºåºï¼‰
        results_slot[i - 1] = summary_record


async def main():
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = PROJECT_ROOT / "eval" / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    log_path = log_dir / f"agentic_rag_{timestamp}.log"
    detail_path = log_dir / f"agentic_rag_{timestamp}_detail.jsonl"

    with open(log_path, "w", encoding="utf-8") as lf, \
         open(detail_path, "w", encoding="utf-8") as df:

        mode = "MCP + Grep/Glob/Read" if USE_MCP else "Grep/Glob/Read (æ—  MCP)"
        model_info = f"direct â†’ {MODEL_NAME or 'default'} ({DIRECT_BASE_URL})" if USE_DIRECT else (f"via router â†’ {ROUTER_MODEL or 'default'} ({ROUTER_URL})" if USE_ROUTER else "Claude (direct)")
        kb_commit_header = get_kb_commit()
        log("=" * 80, lf)
        log(f"ğŸ¤– Agentic RAG æµ‹è¯• (Agent SDK)", lf)
        log("=" * 80, lf)
        log(f"ç”¨ä¾‹: {len(TEST_CASES)} | æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", lf)
        log(f"æ¨¡å¼: {mode}", lf)
        log(f"æ¨¡å‹: {model_info}", lf)
        log(f"å¹¶å‘: {EVAL_CONCURRENCY}", lf)
        log(f"KB commit: {kb_commit_header}", lf)
        log(f"è¯„ä¼°: eval_module (Gate é—¨ç¦ + è´¨é‡æ£€æŸ¥)", lf)
        log(f"ç­–ç•¥: Claude è‡ªä¸»é€‰æ‹©æ£€ç´¢ç­–ç•¥ (Grep/Glob/Read{' + MCP hybrid_search' if USE_MCP else ''})", lf)
        log(f"æ—¥å¿—: {log_path}", lf)
        log(f"è¯¦ç»†: {detail_path}", lf)
        log("", lf)

        # é¢„åˆ†é…ç»“æœ slotï¼ˆä¿æŒé¡ºåºï¼‰
        results_slot = [None] * len(TEST_CASES)
        sem = asyncio.Semaphore(EVAL_CONCURRENCY)
        log_lock = threading.Lock()

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ç”¨ä¾‹
        tasks = [
            run_single_case(i, tc, sem, results_slot, log_lock, lf, df)
            for i, tc in enumerate(TEST_CASES, 1)
        ]
        await asyncio.gather(*tasks)

        # æ”¶é›†ç»“æœ
        results = [r for r in results_slot if r is not None]
        passed = sum(1 for r in results if r["status"] == "passed")
        failed = sum(1 for r in results if r["status"] == "failed")
        errors = sum(1 for r in results if r["status"] == "error")
        total_time = sum(r["elapsed_seconds"] for r in results)
        total_cost = sum(r["cost_usd"] for r in results)

        # æ€»ç»“
        total = len(TEST_CASES)
        log(f"\n{'=' * 80}", lf)
        log(f"ğŸ“Š æ€»ç»“: âœ…{passed} âŒ{failed} âš ï¸{errors} / {total} ({passed/total*100:.1f}%)", lf)
        log(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}s | å¹³å‡: {total_time/max(passed+failed,1):.1f}s", lf)
        log(f"ğŸ’° æ€»è´¹ç”¨: ${total_cost:.4f}", lf)

        # é€Ÿåº¦ç»Ÿè®¡: avg/p50/p95 latency
        elapsed_list = sorted([r["elapsed_seconds"] for r in results if r["elapsed_seconds"] > 0])
        if elapsed_list:
            avg_latency = sum(elapsed_list) / len(elapsed_list)
            p50_idx = max(0, int(len(elapsed_list) * 0.5) - 1)
            p95_idx = max(0, int(len(elapsed_list) * 0.95) - 1)
            p50_latency = elapsed_list[p50_idx]
            p95_latency = elapsed_list[p95_idx]
            min_latency = elapsed_list[0]
            max_latency = elapsed_list[-1]
            log(f"â±ï¸  å»¶è¿Ÿ: avg={avg_latency:.1f}s | p50={p50_latency:.1f}s | p95={p95_latency:.1f}s | min={min_latency:.1f}s | max={max_latency:.1f}s", lf)

        # æŒ‰æŸ¥è¯¢ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for r in results:
            qtype = r.get("type", "unknown")
            type_stats.setdefault(qtype, {"t": 0, "p": 0})
            type_stats[qtype]["t"] += 1
            if r["status"] == "passed": type_stats[qtype]["p"] += 1

        log("", lf)
        log("ğŸ“ˆ æŒ‰æŸ¥è¯¢ç±»å‹:", lf)
        type_labels = {
            "exact": "ç²¾ç¡®åŒ¹é…(Grepæ“…é•¿)",
            "scenario": "SOåœºæ™¯/ç—‡çŠ¶æè¿°",
            "cross-lang": "è·¨è¯­è¨€",
            "howto": "å®æ“é—®ç­”",
            "multi-doc": "å¤šæ–‡æ¡£ç»¼åˆ",
            "concept": "æ¦‚å¿µå‹(éœ€Qdrant)",
            "notfound": "æœªæ”¶å½•",
        }
        for t, s in sorted(type_stats.items()):
            r2 = s["p"]/s["t"]*100
            label = type_labels.get(t, t)
            log(f"  {'âœ…' if r2==100 else 'âš ï¸' if r2>0 else 'âŒ'} {label}: {s['p']}/{s['t']} ({r2:.0f}%)", lf)

        log("", lf)
        log("ğŸ“‹ æŒ‰ category:", lf)
        cats = {}
        for r in results:
            c = r["category"]
            cats.setdefault(c, {"t": 0, "p": 0})
            cats[c]["t"] += 1
            if r["status"] == "passed": cats[c]["p"] += 1
        for c, s in sorted(cats.items()):
            r2 = s["p"]/s["t"]*100
            log(f"  {'âœ…' if r2==100 else 'âš ï¸' if r2>0 else 'âŒ'} {c}: {s['p']}/{s['t']} ({r2:.0f}%)", lf)

        # æŒ‰æ•°æ®æºç»Ÿè®¡
        source_stats = {}
        for r in results:
            src = r.get("source", "unknown")
            source_stats.setdefault(src, {"t": 0, "p": 0})
            source_stats[src]["t"] += 1
            if r["status"] == "passed": source_stats[src]["p"] += 1

        log("", lf)
        log("ğŸ“ˆ æŒ‰æ•°æ®æº:", lf)
        source_labels = {
            "local": "æœ¬åœ° docs/ (Grep/Glob/Read)",
            "qdrant": "Qdrant ç´¢å¼• (MCP hybrid_search)",
            "notfound": "æœªæ”¶å½• (åº”æ‹’ç­”)",
        }
        for src, s in sorted(source_stats.items()):
            r2 = s["p"]/s["t"]*100
            label = source_labels.get(src, src)
            icon = "âœ…" if r2 == 100 else ("âš ï¸" if r2 > 0 else "âŒ")
            log(f"  {icon} {label}: {s['p']}/{s['t']} ({r2:.0f}%)", lf)

        if not USE_MCP and source_stats.get("qdrant", {}).get("t", 0) > 0:
            qdrant_total = source_stats["qdrant"]["t"]
            qdrant_pass = source_stats["qdrant"]["p"]
            log(f"\n  âš ï¸  æ³¨æ„: {qdrant_total} ä¸ª Qdrant ç”¨ä¾‹åœ¨æ—  MCP æ¨¡å¼ä¸‹è¿è¡Œ", lf)
            log(f"     é€šè¿‡ {qdrant_pass}/{qdrant_total} â€” å¯èƒ½æ˜¯ Claude ç”¨é€šç”¨çŸ¥è¯†å›ç­”ï¼ˆéæ£€ç´¢ï¼‰", lf)
            log(f"     è®¾ç½® USE_MCP=1 å¯ç”¨ hybrid_search ä»¥æµ‹è¯•çœŸæ­£çš„å‘é‡æ£€ç´¢", lf)

        # â”€â”€ Early Stopping è§‚æµ‹æŒ‡æ ‡ï¼ˆçº¯åå¤„ç†ï¼Œä¸å½±å“ Agent è¡Œä¸ºï¼‰â”€â”€
        def _compute_early_stop_metrics(r):
            """ä» turn_timings ç»Ÿè®¡æ£€ç´¢æ•ˆç‡æŒ‡æ ‡ã€‚"""
            timings = r.get("turn_timings", [])
            tools = r.get("tools_used", [])

            # search_calls: hybrid_search è°ƒç”¨æ¬¡æ•°
            search_calls = sum(1 for t in tools if "hybrid_search" in t)

            # max_consecutive_same_tool: æœ€å¤§è¿ç»­åŒå·¥å…·æ¬¡æ•°ï¼ˆRead ä¸åŒæ–‡ä»¶è±å…ï¼‰
            max_consec = 1
            cur_consec = 1
            for j in range(1, len(tools)):
                t_cur = tools[j]
                t_prev = tools[j - 1]
                if t_cur == t_prev:
                    # Read ä¸åŒæ–‡ä»¶è±å…ï¼šæ£€æŸ¥ turn_timings ä¸­çš„ tool_input
                    if t_cur == "Read" and j < len(timings) and j - 1 < len(timings):
                        path_cur = timings[j].get("tool_input", {}).get("file_path", "") if isinstance(timings[j].get("tool_input"), dict) else ""
                        path_prev = timings[j-1].get("tool_input", {}).get("file_path", "") if isinstance(timings[j-1].get("tool_input"), dict) else ""
                        if path_cur != path_prev:
                            cur_consec = 1
                            continue
                    cur_consec += 1
                    max_consec = max(max_consec, cur_consec)
                else:
                    cur_consec = 1

            # stop_reason: ä»ç»“æœæ¨æ–­
            num_turns = r.get("num_turns", 0)
            status = r.get("status", "")
            answer = r.get("answer_preview", "") or ""
            if num_turns >= 10:
                stop_reason = "max_turns_reached"
            elif "âŒ" in answer and ("æœªæ‰¾åˆ°" in answer or "not found" in answer.lower()):
                stop_reason = "notfound_quick" if num_turns <= 4 else "notfound_slow"
            elif search_calls <= 1 and num_turns <= 4:
                stop_reason = "hit_first_search"
            elif max_consec >= 3:
                stop_reason = "loop_detected"
            else:
                stop_reason = "normal"

            return {
                "search_calls": search_calls,
                "max_consecutive_same_tool": max_consec,
                "stop_reason": stop_reason,
            }

        # è®¡ç®—æ¯ä¸ª case çš„ early stop æŒ‡æ ‡
        for r in results:
            r["early_stop"] = _compute_early_stop_metrics(r)

        # æ±‡æ€» early stop ç»Ÿè®¡
        stop_reasons = {}
        all_search_calls = []
        all_max_consec = []
        for r in results:
            es = r.get("early_stop", {})
            reason = es.get("stop_reason", "unknown")
            stop_reasons[reason] = stop_reasons.get(reason, 0) + 1
            all_search_calls.append(es.get("search_calls", 0))
            all_max_consec.append(es.get("max_consecutive_same_tool", 0))

        avg_search = sum(all_search_calls) / max(len(all_search_calls), 1)
        avg_consec = sum(all_max_consec) / max(len(all_max_consec), 1)

        # Turns ç»Ÿè®¡
        all_turns = [r.get("num_turns", 0) for r in results if r.get("num_turns", 0) > 0]
        avg_turns = sum(all_turns) / max(len(all_turns), 1)
        high_turn_cases = [r for r in results if r.get("num_turns", 0) >= 7]

        log("", lf)
        log("ğŸ” æ£€ç´¢æ•ˆç‡ (Early Stopping):", lf)
        log(f"  avg turns: {avg_turns:.1f} | avg search_calls: {avg_search:.1f} | avg max_consec_same_tool: {avg_consec:.1f}", lf)
        log(f"  stop_reasons: {json.dumps(stop_reasons, ensure_ascii=False)}", lf)
        if high_turn_cases:
            log(f"  âš ï¸ é«˜ turn cases (â‰¥7): {len(high_turn_cases)}", lf)
            for r in sorted(high_turn_cases, key=lambda x: -x.get("num_turns", 0))[:5]:
                log(f"    - {r['test_id']}: {r.get('num_turns', 0)} turns | {r.get('elapsed_seconds', 0):.0f}s | {r.get('early_stop', {}).get('stop_reason', '?')}", lf)

        # LLM Judge ç»Ÿè®¡
        if USE_JUDGE:
            judge_scores = [r.get("judge_score") for r in results
                           if r.get("judge_score") is not None]
            if judge_scores:
                avg_score = sum(judge_scores) / len(judge_scores)
                faith_vals = [r.get("faithfulness", 0) for r in results
                              if r.get("faithfulness") is not None and r.get("faithfulness", -1) >= 0]
                rel_vals = [r.get("relevancy", 0) for r in results
                            if r.get("relevancy") is not None and r.get("relevancy", -1) >= 0]
                ctx_prec_vals = [r.get("context_precision", 0) for r in results
                                 if r.get("context_precision") is not None and r.get("context_precision", -1) >= 0]
                ctx_rec_vals = [r.get("context_recall", 0) for r in results
                                if r.get("context_recall") is not None and r.get("context_recall", -1) >= 0]
                correct_vals = [r.get("answer_correctness", 0) for r in results
                                if r.get("answer_correctness") is not None and r.get("answer_correctness", -1) >= 0]
                avg_faith = sum(faith_vals) / max(len(faith_vals), 1)
                avg_rel = sum(rel_vals) / max(len(rel_vals), 1)
                low_quality = [r for r in results if r.get("judge_score") is not None and r["judge_score"] < 3]
                log("", lf)
                log(f"ğŸ§‘â€âš–ï¸ LLM Judge ({len(judge_scores)} cases):", lf)
                log(f"  å¹³å‡ score: {avg_score:.2f}/5 | faithfulness: {avg_faith:.3f} | relevancy: {avg_rel:.3f}", lf)
                if ctx_prec_vals:
                    avg_ctx_prec = sum(ctx_prec_vals) / len(ctx_prec_vals)
                    log(f"  context_precision: {avg_ctx_prec:.3f} ({len(ctx_prec_vals)} cases with reference_answer)", lf)
                if ctx_rec_vals:
                    avg_ctx_rec = sum(ctx_rec_vals) / len(ctx_rec_vals)
                    log(f"  context_recall: {avg_ctx_rec:.3f} ({len(ctx_rec_vals)} cases with reference_answer)", lf)
                if correct_vals:
                    avg_correct = sum(correct_vals) / len(correct_vals)
                    log(f"  answer_correctness: {avg_correct:.3f} ({len(correct_vals)} cases with reference_answer)", lf)
                if low_quality:
                    log(f"  âš ï¸ ä½è´¨é‡ (score<3): {len(low_quality)} ä¸ª", lf)
                    for r in low_quality[:5]:
                        log(f"    - {r['test_id']}: score={r['judge_score']} {r.get('query', '')[:40]}", lf)

        # â”€â”€ è´¨é‡æŒ‡æ ‡ï¼ˆç‹¬ç«‹äº gateï¼Œè¡¡é‡å›ç­”è´¨é‡ï¼‰â”€â”€
        # åªç»Ÿè®¡ gate_passed=True ä¸”æœ‰ judge åˆ†æ•°çš„ case
        judged_results = [r for r in results if r.get("gate_passed") and r.get("faithfulness") is not None]
        if judged_results:
            faith_ge_05 = sum(1 for r in judged_results if (r.get("faithfulness") or 0) >= 0.5)
            judge_ge_3 = sum(1 for r in judged_results if (r.get("judge_score") or 0) >= 3.0)
            pct_faith = faith_ge_05 / len(judged_results) * 100
            pct_judge = judge_ge_3 / len(judged_results) * 100
            log("", lf)
            log(f"ğŸ“Š è´¨é‡æŒ‡æ ‡ ({len(judged_results)} judged cases):", lf)
            log(f"  pct_faith_ge_05: {faith_ge_05}/{len(judged_results)} ({pct_faith:.1f}%)", lf)
            log(f"  pct_judge_ge_3:  {judge_ge_3}/{len(judged_results)} ({pct_judge:.1f}%)", lf)
        else:
            pct_faith = None
            pct_judge = None

        # â”€â”€ è¶Šç•Œæ£€æµ‹ï¼ˆboundary violationï¼‰â”€â”€
        BOUNDARY_PREFIXES = [
            "tests/", "eval/", "scripts/",
            "docs/ragbench-techqa/", "docs/crag-finance/", "docs/archive/",
        ]
        BOUNDARY_PATTERNS = [
            "docs/design", "docs/dual-", "docs/e2e-", "docs/eval-", "docs/progress-",
        ]
        boundary_violations = []
        for r in results:
            paths = r.get("retrieved_paths", [])
            violated_paths = []
            for p in paths:
                for prefix in BOUNDARY_PREFIXES:
                    if p.startswith(prefix) or f"/{prefix}" in p:
                        violated_paths.append(p)
                        break
                else:
                    for pat in BOUNDARY_PATTERNS:
                        if pat in p:
                            violated_paths.append(p)
                            break
            if violated_paths:
                r["boundary_violation"] = True
                r["violated_paths"] = violated_paths
                boundary_violations.append(r)
            else:
                r["boundary_violation"] = False

        if boundary_violations:
            log("", lf)
            log(f"âš ï¸  è¶Šç•Œæ£€æµ‹: {len(boundary_violations)} cases è®¿é—®äº†éçŸ¥è¯†åº“è·¯å¾„:", lf)
            for r in boundary_violations:
                log(f"  - {r['test_id']}: {r.get('violated_paths', [])[:3]}", lf)

        # ä¿å­˜æ±‡æ€» JSON
        kb_commit = get_kb_commit()
        out_dir = PROJECT_ROOT / "eval"
        out_file = out_dir / f"agentic_rag_v5_{timestamp}.json"

        # Judge æ±‡æ€»
        judge_summary = {}
        if USE_JUDGE:
            judge_scores = [r.get("judge_score") for r in results
                           if r.get("judge_score") is not None]
            if judge_scores:
                faith_vals = [r.get("faithfulness") for r in results
                              if r.get("faithfulness") is not None and r.get("faithfulness", -1) >= 0]
                rel_vals = [r.get("relevancy") for r in results
                            if r.get("relevancy") is not None and r.get("relevancy", -1) >= 0]
                ctx_prec_vals = [r.get("context_precision") for r in results
                                 if r.get("context_precision") is not None and r.get("context_precision", -1) >= 0]
                ctx_rec_vals = [r.get("context_recall") for r in results
                                if r.get("context_recall") is not None and r.get("context_recall", -1) >= 0]
                correct_vals = [r.get("answer_correctness") for r in results
                                if r.get("answer_correctness") is not None and r.get("answer_correctness", -1) >= 0]
                judge_summary = {
                    "count": len(judge_scores),
                    "avg_score": round(sum(judge_scores) / len(judge_scores), 2),
                    "avg_faithfulness": round(sum(faith_vals) / max(len(faith_vals), 1), 3) if faith_vals else None,
                    "avg_relevancy": round(sum(rel_vals) / max(len(rel_vals), 1), 3) if rel_vals else None,
                    "avg_context_precision": round(sum(ctx_prec_vals) / len(ctx_prec_vals), 3) if ctx_prec_vals else None,
                    "avg_context_recall": round(sum(ctx_rec_vals) / len(ctx_rec_vals), 3) if ctx_rec_vals else None,
                    "avg_answer_correctness": round(sum(correct_vals) / len(correct_vals), 3) if correct_vals else None,
                    "low_quality_count": sum(1 for s in judge_scores if s < 3),
                }

        # é€Ÿåº¦æ±‡æ€»
        speed_summary = {}
        if elapsed_list:
            speed_summary = {
                "avg_seconds": round(avg_latency, 1),
                "p50_seconds": round(p50_latency, 1),
                "p95_seconds": round(p95_latency, 1),
                "min_seconds": round(min_latency, 1),
                "max_seconds": round(max_latency, 1),
            }

        with open(out_file, "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(), "test_type": "agentic_rag_v5",
                "method": "claude_agent_sdk", "total": total,
                "passed": passed, "failed": failed, "errors": errors,
                "total_time": total_time, "total_cost": total_cost,
                "kb_commit": kb_commit,
                "eval_module": "eval_module.py (gate + quality + judge)",
                "model": MODEL_NAME if USE_DIRECT and MODEL_NAME else (ROUTER_MODEL if USE_ROUTER and ROUTER_MODEL else ("router:default" if USE_ROUTER else "claude-sonnet")),
                "dataset": EVAL_DATASET,
                "category_stats": {c: {"total": s["t"], "passed": s["p"]} for c, s in cats.items()},
                "source_stats": {s: {"total": v["t"], "passed": v["p"]} for s, v in source_stats.items()},
                "judge_summary": judge_summary,
                "quality_metrics": {
                    "pct_faith_ge_05": round(pct_faith, 1) if pct_faith is not None else None,
                    "pct_judge_ge_3": round(pct_judge, 1) if pct_judge is not None else None,
                    "judged_count": len(judged_results) if judged_results else 0,
                },
                "boundary_violations": {
                    "count": len(boundary_violations),
                    "cases": [{"test_id": r["test_id"], "violated_paths": r.get("violated_paths", [])} for r in boundary_violations],
                },
                "speed_summary": speed_summary,
                "early_stop_summary": {
                    "avg_turns": round(avg_turns, 1),
                    "avg_search_calls": round(avg_search, 1),
                    "avg_max_consec_same_tool": round(avg_consec, 1),
                    "stop_reasons": stop_reasons,
                    "high_turn_count": len(high_turn_cases),
                },
                "use_mcp": USE_MCP,
                "use_judge": USE_JUDGE,
                "use_router": USE_ROUTER,
                "use_direct": USE_DIRECT,
                "direct_base_url": DIRECT_BASE_URL if USE_DIRECT else None,
                "concurrency": EVAL_CONCURRENCY,
                "results": results,
            }, f, indent=2, ensure_ascii=False)

        log(f"\nğŸ“ æ±‡æ€»: {out_file}", lf)
        log(f"ğŸ“‹ æ—¥å¿—: {log_path}", lf)
        log(f"ğŸ“ è¯¦ç»†: {detail_path}", lf)

    return results


if __name__ == "__main__":
    asyncio.run(main())
