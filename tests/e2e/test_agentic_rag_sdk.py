#!/usr/bin/env python3
"""Agentic RAG è‡ªåŠ¨åŒ–æµ‹è¯• v5 â€” 100 ä¸ªçœŸå®é—®é¢˜

æ•°æ®æº: redis-docs (234 docs) + awesome-llm-apps (207 docs) + local docs/ (3 docs)
è¯„ä¼°: eval_module (Gate é—¨ç¦ + è´¨é‡æ£€æŸ¥ + LLM-as-Judge)
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
load_dotenv()

from claude_agent_sdk import query, ClaudeAgentOptions

PROJECT_ROOT = Path(__file__).parent.parent.parent

# å¯¼å…¥è¯„æµ‹æ¨¡å—
sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
from eval_module import extract_contexts, gate_check, get_tools_used, get_retrieved_doc_paths, get_kb_commit, llm_judge

# å¯¼å…¥ v5 æµ‹è¯•ç”¨ä¾‹
sys.path.insert(0, str(PROJECT_ROOT / "tests" / "fixtures"))
from v5_test_queries import TEST_CASES_V5

TEST_CASES = TEST_CASES_V5

# ä» v5 ç”¨ä¾‹çš„ expected_keywords åŠ¨æ€æ„å»º KEYWORD_CHECKS
KEYWORD_CHECKS = {}
for _tc in TEST_CASES:
    cat = _tc["category"]
    kws = _tc.get("expected_keywords", [])
    if cat not in KEYWORD_CHECKS:
        KEYWORD_CHECKS[cat] = list(kws)
    else:
        for kw in kws:
            if kw not in KEYWORD_CHECKS[cat]:
                KEYWORD_CHECKS[cat].append(kw)

# æ˜¯å¦å¯ç”¨ MCPï¼ˆæ¨¡å‹åŠ è½½éœ€è¦ 15-20 åˆ†é’Ÿï¼Œå¯é€‰å…³é—­ï¼‰
USE_MCP = os.environ.get("USE_MCP", "0") == "1"
# æ˜¯å¦å¯ç”¨ LLM-as-Judgeï¼ˆå¯¹ Gate é€šè¿‡çš„ç”¨ä¾‹åšè´¨é‡æ‰“åˆ†ï¼‰
USE_JUDGE = os.environ.get("USE_JUDGE", "0") == "1"

if USE_MCP:
    BASE_OPTIONS = dict(
        allowed_tools=[
            "Read", "Grep", "Glob", "Bash",
            "mcp__knowledge-base__hybrid_search",
            "mcp__knowledge-base__keyword_search",
            "mcp__knowledge-base__index_status",
        ],
        mcp_servers={
            "knowledge-base": {
                "command": str(PROJECT_ROOT / ".venv" / "bin" / "python"),
                "args": [str(PROJECT_ROOT / "scripts" / "mcp_server.py")],
                "env": {
                    "QDRANT_URL": os.environ.get("QDRANT_URL", "http://localhost:6333"),
                    "COLLECTION_NAME": os.environ.get("COLLECTION_NAME", "knowledge-base"),
                },
            }
        },
        setting_sources=["project"],
        permission_mode="bypassPermissions",
        cwd=str(PROJECT_ROOT),
        max_turns=15,
    )
else:
    # æ—  MCP æ¨¡å¼ï¼šä»…ä½¿ç”¨ Grep/Glob/Readï¼ˆAgentic Layer 1ï¼‰
    # æ³¨æ„ï¼šä¸èƒ½ç”¨ setting_sources=["project"]ï¼Œå¦åˆ™ä¼šåŠ è½½ .mcp.json å¯åŠ¨ MCP server
    # æ”¹ç”¨ system_prompt æ³¨å…¥ CLAUDE.md å’Œ search skill çš„å…³é”®æŒ‡ä»¤
    BASE_OPTIONS = dict(
        allowed_tools=["Read", "Grep", "Glob", "Bash"],
        permission_mode="bypassPermissions",
        cwd=str(PROJECT_ROOT),
        max_turns=15,
        system_prompt="""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“æ£€ç´¢åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šç”¨ /search å‘½ä»¤æŸ¥è¯¢çŸ¥è¯†åº“ã€‚

çŸ¥è¯†åº“æ–‡æ¡£åœ¨ docs/ ç›®å½•ä¸‹ï¼Œæ ¼å¼ä¸º Markdownï¼ŒåŒ…å« Redisã€LLM/AI åº”ç”¨å¼€å‘ç­‰æŠ€æœ¯æ–‡æ¡£ã€‚

æ£€ç´¢ç­–ç•¥ï¼š
1. ä½¿ç”¨ Grep æœç´¢å…³é”®è¯
2. ä½¿ç”¨ Glob æŸ¥æ‰¾ç›¸å…³æ–‡ä»¶ï¼ˆå¦‚ docs/**/*.mdï¼‰
3. ä½¿ç”¨ Read è¯»å–å‘½ä¸­æ–‡ä»¶çš„ç›¸å…³æ®µè½
4. ç»¼åˆåˆ†æå¹¶å›ç­”ï¼Œå¿…é¡»å¸¦å¼•ç”¨ [æ¥æº: docs/xxx.md]

å›ç­”è¦æ±‚ï¼š
- å¿…é¡»ä¸”åªèƒ½åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œåªå›ç­”"æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"ï¼Œä¸è¦æä¾›ä»»ä½•å»ºè®®ã€æ›¿ä»£æ–¹æ¡ˆæˆ–é€šç”¨çŸ¥è¯†
- ä¸¥ç¦ç”¨ä½ è‡ªå·±çš„è®­ç»ƒçŸ¥è¯†è¡¥å……å›ç­”ã€‚å¦‚æœ docs/ ä¸­æ²¡æœ‰ï¼Œå°±æ˜¯æ²¡æœ‰
- å›ç­”è¯­è¨€è·ŸéšæŸ¥è¯¢è¯­è¨€ï¼ˆä¸­æ–‡é—®ä¸­æ–‡ç­”ï¼Œè‹±æ–‡é—®è‹±æ–‡ç­”ï¼‰
- å¼•ç”¨å…·ä½“æ–‡æ¡£è·¯å¾„
""",
    )


def log(msg: str, log_file=None):
    """åŒæ—¶è¾“å‡ºåˆ° stdout å’Œæ—¥å¿—æ–‡ä»¶"""
    print(msg, flush=True)
    if log_file:
        log_file.write(msg + "\n")
        log_file.flush()


async def run_query(prompt: str, session_id: Optional[str], log_file) -> Dict[str, Any]:
    """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢ï¼Œå®Œæ•´è®°å½•ä¸­é—´è¿‡ç¨‹"""
    start = time.time()
    answer = ""
    new_session_id = session_id
    cost = 0.0
    num_turns = 0
    tools_used = []
    messages_log = []  # å®Œæ•´æ¶ˆæ¯æ—¥å¿—

    try:
        if session_id:
            opts = ClaudeAgentOptions(
                resume=session_id,
                permission_mode="bypassPermissions",
                max_turns=15,
            )
        else:
            opts = ClaudeAgentOptions(**BASE_OPTIONS)

        async for msg in query(prompt=prompt, options=opts):
            # è®°å½•æ¯æ¡æ¶ˆæ¯çš„å®Œæ•´ä¿¡æ¯
            msg_dict = {}
            for attr in ["type", "subtype", "role", "result", "session_id",
                         "cost_usd", "num_turns", "tool_name", "tool_input",
                         "tool_result", "content", "text", "data",
                         "stop_reason", "duration_ms", "total_cost_usd",
                         "usage", "modelUsage", "permission_denials"]:
                val = getattr(msg, attr, None)
                if val is not None:
                    try:
                        json.dumps(val)  # ç¡®ä¿å¯åºåˆ—åŒ–
                        msg_dict[attr] = val
                    except (TypeError, ValueError):
                        msg_dict[attr] = str(val)

            messages_log.append(msg_dict)

            # è¯¦ç»†æ—¥å¿—è¾“å‡º - è§£æ SDK æ¶ˆæ¯ç»“æ„
            subtype = getattr(msg, "subtype", None)
            content = getattr(msg, "content", None)

            if subtype == "init":
                data = getattr(msg, "data", {}) or {}
                if isinstance(data, dict):
                    new_session_id = data.get("session_id", new_session_id)
                    model = data.get("model", "unknown")
                    log(f"    [INIT] session={new_session_id} model={model}", log_file)
                if hasattr(msg, "session_id"):
                    new_session_id = msg.session_id

            elif subtype == "success":
                # æœ€ç»ˆç»“æœ
                if hasattr(msg, "result"):
                    answer = msg.result if isinstance(msg.result, str) else str(msg.result)
                num_turns = getattr(msg, "num_turns", num_turns)
                cost = getattr(msg, "total_cost_usd", cost) or cost
                duration = getattr(msg, "duration_ms", 0)
                log(f"    [RESULT] {len(answer)} chars | {num_turns} turns | {duration}ms", log_file)

            elif content:
                # è§£æ content blocksï¼ˆTextBlock, ToolUseBlock, ToolResultBlockï¼‰
                if isinstance(content, list):
                    for block in content:
                        block_type = getattr(block, "type", None)
                        if block_type is None and isinstance(block, dict):
                            block_type = block.get("type", "")

                        if block_type == "text" or (hasattr(block, "text") and not hasattr(block, "name") and not hasattr(block, "tool_use_id")):
                            # TextBlock - Claude çš„æ¨ç†/å›å¤
                            text = getattr(block, "text", "") if hasattr(block, "text") else block.get("text", "")
                            if text:
                                # å¤šè¡Œæ–‡æœ¬æˆªæ–­æ˜¾ç¤º
                                lines = text.strip().split("\n")
                                preview = lines[0][:200]
                                if len(lines) > 1:
                                    preview += f" (+{len(lines)-1} lines)"
                                log(f"    [THINK] {preview}", log_file)

                        elif block_type == "tool_use" or hasattr(block, "name"):
                            # ToolUseBlock - å·¥å…·è°ƒç”¨
                            tool_name = getattr(block, "name", "unknown")
                            tool_input = getattr(block, "input", {})
                            if isinstance(tool_input, dict):
                                input_str = json.dumps(tool_input, ensure_ascii=False)
                            else:
                                input_str = str(tool_input)
                            if len(input_str) > 300:
                                input_str = input_str[:300] + "..."
                            log(f"    [TOOL] {tool_name}({input_str})", log_file)
                            tools_used.append(tool_name)

                        elif block_type == "tool_result" or hasattr(block, "tool_use_id"):
                            # ToolResultBlock - å·¥å…·è¿”å›
                            result_content = getattr(block, "content", "")
                            if isinstance(result_content, str):
                                # æˆªæ–­é•¿ç»“æœï¼Œä¿ç•™å…³é”®ä¿¡æ¯
                                lines = result_content.strip().split("\n")
                                if len(lines) > 5:
                                    preview = "\n".join(lines[:3]) + f"\n    ... ({len(lines)} lines total)"
                                else:
                                    preview = result_content[:400]
                                log(f"    [TOOL_OUT] {preview}", log_file)
                            elif isinstance(result_content, list):
                                for rc in result_content:
                                    rc_text = getattr(rc, "text", str(rc)) if hasattr(rc, "text") else str(rc)
                                    log(f"    [TOOL_OUT] {rc_text[:300]}", log_file)

            # æå–å…ƒæ•°æ®
            if hasattr(msg, "cost_usd") and msg.cost_usd:
                cost = msg.cost_usd
            if hasattr(msg, "total_cost_usd") and msg.total_cost_usd:
                cost = msg.total_cost_usd
            if hasattr(msg, "num_turns") and msg.num_turns:
                num_turns = msg.num_turns

            # æ£€æŸ¥ permission denials
            denials = getattr(msg, "permission_denials", None)
            if denials:
                for d in denials:
                    tool = d.get("tool_name", "unknown") if isinstance(d, dict) else str(d)
                    log(f"    [DENIED] {tool}", log_file)

        elapsed = time.time() - start
        return {
            "status": "success",
            "answer": answer,
            "session_id": new_session_id,
            "elapsed": elapsed,
            "cost_usd": cost,
            "num_turns": num_turns,
            "tools_used": tools_used,
            "messages_log": messages_log,
        }
    except Exception as e:
        elapsed = time.time() - start
        log(f"    [ERROR] {e}", log_file)
        return {
            "status": "error",
            "error": str(e),
            "session_id": new_session_id,
            "elapsed": elapsed,
            "messages_log": messages_log,
        }


def evaluate(tc: Dict, result: Dict) -> Dict:
    """ä¸¤é˜¶æ®µè¯„ä¼°: Gate é—¨ç¦ (ç¡®å®šæ€§) â†’ å…³é”®è¯è´¨é‡æ£€æŸ¥ (è¾…åŠ©)ã€‚"""
    ev = {"passed": False, "reasons": [], "quality": {}, "gate": {}}

    if result["status"] != "success":
        ev["reasons"].append(f"æ‰§è¡Œå¤±è´¥: {result.get('error', '')[:80]}")
        return ev

    answer = result.get("answer", "")
    messages_log = result.get("messages_log", [])

    # â”€â”€ Stage 1: ç»“æ„åŒ– context æå– â”€â”€
    contexts = extract_contexts(messages_log)
    ev["quality"]["contexts_count"] = len(contexts)
    ev["quality"]["tools_used"] = get_tools_used(contexts)
    ev["quality"]["retrieved_paths"] = get_retrieved_doc_paths(contexts)

    # â”€â”€ Stage 2: Gate é—¨ç¦ â”€â”€
    gate = gate_check(tc, answer, contexts)
    ev["gate"] = gate

    if not gate["passed"]:
        ev["passed"] = False
        ev["reasons"] = gate["reasons"]
        return ev

    # â”€â”€ Stage 3: è´¨é‡æ£€æŸ¥ (Gate é€šè¿‡åçš„è¾…åŠ©æŒ‡æ ‡) â”€â”€
    if len(answer) < 50:
        ev["reasons"].append(f"ç­”æ¡ˆè¿‡çŸ­ ({len(answer)})")
        return ev

    # å¼•ç”¨è´¨é‡ (ä» gate è·å–)
    ev["quality"]["has_citation"] = gate["checks"].get("has_citation", False)

    # å…³é”®è¯åŒ¹é… (è¾…åŠ©ä¿¡å·ï¼Œä¸ä½œä¸º pass/fail åˆ¤æ®)
    expected = KEYWORD_CHECKS.get(tc["category"], [])
    matched = [k for k in expected if k.lower() in answer.lower()]
    ev["quality"]["keywords"] = matched

    # æ­£ç¡®æ–‡æ¡£å¼•ç”¨ (ä» gate çš„ expected_doc_hit è·å–)
    ev["quality"]["correct_doc"] = gate["checks"].get("expected_doc_hit", None)

    # Gate é€šè¿‡ + ç­”æ¡ˆè¶³å¤Ÿé•¿ â†’ é€šè¿‡
    if len(answer) >= 50:
        ev["passed"] = True
    else:
        ev["reasons"].append(f"ç­”æ¡ˆè¿‡çŸ­ ({len(answer)})")
        return ev

    # â”€â”€ Stage 4: LLM-as-Judge (å¯é€‰ï¼ŒGate é€šè¿‡å) â”€â”€
    if USE_JUDGE and ev["passed"] and not tc.get("source") == "notfound":
        judge = llm_judge(tc["query"], answer, contexts)
        ev["quality"]["judge"] = judge
        # Judge score < 2 è§†ä¸ºè´¨é‡ä¸åˆæ ¼ï¼ˆä½†ä¸æ”¹å˜ pass/failï¼‰
        if judge.get("score", -1) >= 0:
            ev["quality"]["judge_score"] = judge["score"]
            ev["quality"]["faithfulness"] = judge.get("faithfulness", -1)
            ev["quality"]["relevancy"] = judge.get("relevancy", -1)

    return ev


async def main():
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = PROJECT_ROOT / "eval" / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)
    log_path = log_dir / f"agentic_rag_{timestamp}.log"
    detail_path = log_dir / f"agentic_rag_{timestamp}_detail.jsonl"

    with open(log_path, "w", encoding="utf-8") as lf, \
         open(detail_path, "w", encoding="utf-8") as df:

        mode = "MCP + Grep/Glob/Read" if USE_MCP else "Grep/Glob/Read (æ—  MCP)"
        kb_commit_header = get_kb_commit()
        log("=" * 80, lf)
        log(f"ğŸ¤– Agentic RAG æµ‹è¯• (Agent SDK)", lf)
        log("=" * 80, lf)
        log(f"ç”¨ä¾‹: {len(TEST_CASES)} | æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", lf)
        log(f"æ¨¡å¼: {mode}", lf)
        log(f"KB commit: {kb_commit_header}", lf)
        log(f"è¯„ä¼°: eval_module (Gate é—¨ç¦ + è´¨é‡æ£€æŸ¥)", lf)
        log(f"ç­–ç•¥: Claude è‡ªä¸»é€‰æ‹©æ£€ç´¢ç­–ç•¥ (Grep/Glob/Read{' + MCP hybrid_search' if USE_MCP else ''})", lf)
        log(f"æ—¥å¿—: {log_path}", lf)
        log(f"è¯¦ç»†: {detail_path}", lf)
        log("", lf)

        results = []
        passed = failed = errors = 0
        total_time = total_cost = 0.0
        # ä¸å¤ç”¨ session â€” æ¯ä¸ªç”¨ä¾‹ç‹¬ç«‹ sessionï¼Œç¡®ä¿ Agent æ¯æ¬¡éƒ½æ‰§è¡Œå·¥å…·è°ƒç”¨
        # å¤ç”¨ session ä¼šå¯¼è‡´ Claude ä»ä¸Šä¸‹æ–‡è®°å¿†å›ç­”ï¼Œè·³è¿‡æ£€ç´¢ï¼Œextract_contexts() ä¸ºç©º

        for i, tc in enumerate(TEST_CASES, 1):
            log(f"\n{'='*60}", lf)
            log(f"[{i}/{len(TEST_CASES)}] {tc['id']} ({tc['category']}) [{tc.get('type', '?')}]", lf)
            log(f"  Q: {tc['query']}", lf)
            if tc.get("note"):
                log(f"  ğŸ’¡ {tc['note']}", lf)
            if i == 1 and USE_MCP:
                log(f"  â³ é¦–æ¬¡æŸ¥è¯¢ï¼ŒåŠ è½½ MCP server (BGE-M3)...", lf)
            log(f"  å¼€å§‹: {datetime.now().strftime('%H:%M:%S')}", lf)

            # å¦‚æœæœ‰ MCP + skillsï¼Œç”¨ /searchï¼›å¦åˆ™ç›´æ¥æé—®
            prompt = f"/search {tc['query']}" if USE_MCP else f"è¯·åœ¨ docs/ ç›®å½•ä¸­æ£€ç´¢å¹¶å›ç­”: {tc['query']}"
            result = await run_query(prompt, None, lf)  # None = æ¯æ¬¡æ–° session

            elapsed = result.get("elapsed", 0)
            total_time += elapsed
            total_cost += result.get("cost_usd", 0)

            ev = evaluate(tc, result)

            if result["status"] == "error":
                log(f"  âŒ é”™è¯¯: {result.get('error', '')[:80]}", lf)
                errors += 1
                status = "error"
            elif ev["passed"]:
                ans_len = len(result.get("answer", ""))
                quality = ev.get("quality", {})
                tools = quality.get("tools_used", [])
                cite = "å¼•ç”¨âœ…" if quality.get("has_citation") else "å¼•ç”¨âŒ"
                correct_doc = quality.get("correct_doc")
                doc_tag = "æ–‡æ¡£âœ…" if correct_doc else ("æ–‡æ¡£âŒ" if correct_doc is False else "")
                ctx_count = quality.get("contexts_count", 0)
                kw = quality.get("keywords", [])
                log(f"  âœ… é€šè¿‡ | {ans_len}å­—ç¬¦ | {elapsed:.1f}s | ${result.get('cost_usd', 0):.4f} | {cite} {doc_tag} | ctx:{ctx_count}", lf)
                if tools:
                    log(f"  ğŸ”§ å·¥å…·: {', '.join(tools)}", lf)
                if quality.get("retrieved_paths"):
                    log(f"  ğŸ“„ æ£€ç´¢: {', '.join(quality['retrieved_paths'][:5])}", lf)
                if kw:
                    log(f"  ğŸ”‘ å…³é”®è¯: {', '.join(kw)}", lf)
                if quality.get("judge_score") is not None:
                    js = quality["judge_score"]
                    ff = quality.get("faithfulness", "?")
                    rr = quality.get("relevancy", "?")
                    log(f"  ğŸ§‘â€âš–ï¸ Judge: score={js} faith={ff} rel={rr}", lf)
                passed += 1
                status = "passed"
            else:
                log(f"  âŒ å¤±è´¥: {'; '.join(ev['reasons'])}", lf)
                failed += 1
                status = "failed"

            # è¾“å‡ºç­”æ¡ˆé¢„è§ˆï¼ˆé€šè¿‡å’Œå¤±è´¥éƒ½è¾“å‡ºï¼‰
            ans_preview = result.get("answer", "")[:500]
            if ans_preview:
                log(f"  ğŸ“ ç­”æ¡ˆ: {ans_preview}{'...' if len(result.get('answer',''))>500 else ''}", lf)

            log(f"  ç»“æŸ: {datetime.now().strftime('%H:%M:%S')} | è€—æ—¶: {elapsed:.1f}s", lf)

            # å†™å…¥è¯¦ç»† JSONLï¼ˆæ¯ä¸ª query ä¸€è¡Œï¼ŒåŒ…å«å®Œæ•´æ¶ˆæ¯æ—¥å¿—ï¼‰
            gate = ev.get("gate", {})
            quality = ev.get("quality", {})
            detail_record = {
                "test_id": tc["id"],
                "category": tc["category"],
                "type": tc.get("type", "unknown"),
                "source": tc.get("source", "unknown"),
                "query": tc["query"],
                "status": status,
                "elapsed_seconds": elapsed,
                "cost_usd": result.get("cost_usd", 0),
                "num_turns": result.get("num_turns", 0),
                "answer_length": len(result.get("answer", "")),
                "answer": result.get("answer", ""),
                "tools_used": quality.get("tools_used", []),
                "retrieved_paths": quality.get("retrieved_paths", []),
                "contexts_count": quality.get("contexts_count", 0),
                "has_citation": quality.get("has_citation", False),
                "correct_doc": quality.get("correct_doc"),
                "matched_keywords": quality.get("keywords", []),
                "gate_passed": gate.get("passed"),
                "gate_checks": gate.get("checks", {}),
                "failure_reasons": ev.get("reasons", []),
                "judge_score": quality.get("judge_score"),
                "faithfulness": quality.get("faithfulness"),
                "relevancy": quality.get("relevancy"),
                "judge": quality.get("judge"),
                "messages": result.get("messages_log", []),
            }
            df.write(json.dumps(detail_record, ensure_ascii=False) + "\n")
            df.flush()

            results.append({
                "test_id": tc["id"], "category": tc["category"],
                "type": tc.get("type", "unknown"),
                "source": tc.get("source", "unknown"),
                "query": tc["query"],
                "status": status, "elapsed_seconds": elapsed,
                "cost_usd": result.get("cost_usd", 0),
                "num_turns": result.get("num_turns", 0),
                "answer_length": len(result.get("answer", "")),
                "tools_used": quality.get("tools_used", []),
                "retrieved_paths": quality.get("retrieved_paths", []),
                "contexts_count": quality.get("contexts_count", 0),
                "has_citation": quality.get("has_citation", False),
                "correct_doc": quality.get("correct_doc"),
                "matched_keywords": quality.get("keywords", []),
                "gate_passed": gate.get("passed"),
                "failure_reasons": ev.get("reasons", []),
                "answer_preview": result.get("answer", "")[:300],
                "judge_score": quality.get("judge_score"),
                "faithfulness": quality.get("faithfulness"),
                "relevancy": quality.get("relevancy"),
            })
            log("-" * 80, lf)

        # æ€»ç»“
        total = len(TEST_CASES)
        log(f"\n{'=' * 80}", lf)
        log(f"ğŸ“Š æ€»ç»“: âœ…{passed} âŒ{failed} âš ï¸{errors} / {total} ({passed/total*100:.1f}%)", lf)
        log(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f}s | å¹³å‡: {total_time/max(passed+failed,1):.1f}s", lf)
        log(f"ğŸ’° æ€»è´¹ç”¨: ${total_cost:.4f}", lf)

        # æŒ‰æŸ¥è¯¢ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for i2, r in enumerate(results):
            qtype = TEST_CASES[i2].get("type", "unknown")
            type_stats.setdefault(qtype, {"t": 0, "p": 0})
            type_stats[qtype]["t"] += 1
            if r["status"] == "passed": type_stats[qtype]["p"] += 1

        log("", lf)
        log("ğŸ“ˆ æŒ‰æŸ¥è¯¢ç±»å‹:", lf)
        type_labels = {
            "exact": "ç²¾ç¡®åŒ¹é…(Grepæ“…é•¿)",
            "scenario": "SOåœºæ™¯/ç—‡çŠ¶æè¿°",
            "cross-lang": "è·¨è¯­è¨€",
            "howto": "å®æ“é—®ç­”",
            "multi-doc": "å¤šæ–‡æ¡£ç»¼åˆ",
            "concept": "æ¦‚å¿µå‹(éœ€Qdrant)",
            "notfound": "æœªæ”¶å½•",
        }
        for t, s in sorted(type_stats.items()):
            r2 = s["p"]/s["t"]*100
            label = type_labels.get(t, t)
            log(f"  {'âœ…' if r2==100 else 'âš ï¸' if r2>0 else 'âŒ'} {label}: {s['p']}/{s['t']} ({r2:.0f}%)", lf)

        log("", lf)
        log("ğŸ“‹ æŒ‰ category:", lf)
        cats = {}
        for r in results:
            c = r["category"]
            cats.setdefault(c, {"t": 0, "p": 0})
            cats[c]["t"] += 1
            if r["status"] == "passed": cats[c]["p"] += 1
        for c, s in sorted(cats.items()):
            r2 = s["p"]/s["t"]*100
            log(f"  {'âœ…' if r2==100 else 'âš ï¸' if r2>0 else 'âŒ'} {c}: {s['p']}/{s['t']} ({r2:.0f}%)", lf)

        # æŒ‰æ•°æ®æºç»Ÿè®¡
        source_stats = {}
        for i2, r in enumerate(results):
            src = TEST_CASES[i2].get("source", "unknown")
            source_stats.setdefault(src, {"t": 0, "p": 0})
            source_stats[src]["t"] += 1
            if r["status"] == "passed": source_stats[src]["p"] += 1

        log("", lf)
        log("ğŸ“ˆ æŒ‰æ•°æ®æº:", lf)
        source_labels = {
            "local": "æœ¬åœ° docs/ (Grep/Glob/Read)",
            "qdrant": "Qdrant ç´¢å¼• (MCP hybrid_search)",
            "notfound": "æœªæ”¶å½• (åº”æ‹’ç­”)",
        }
        for src, s in sorted(source_stats.items()):
            r2 = s["p"]/s["t"]*100
            label = source_labels.get(src, src)
            icon = "âœ…" if r2 == 100 else ("âš ï¸" if r2 > 0 else "âŒ")
            log(f"  {icon} {label}: {s['p']}/{s['t']} ({r2:.0f}%)", lf)

        if not USE_MCP and source_stats.get("qdrant", {}).get("t", 0) > 0:
            qdrant_total = source_stats["qdrant"]["t"]
            qdrant_pass = source_stats["qdrant"]["p"]
            log(f"\n  âš ï¸  æ³¨æ„: {qdrant_total} ä¸ª Qdrant ç”¨ä¾‹åœ¨æ—  MCP æ¨¡å¼ä¸‹è¿è¡Œ", lf)
            log(f"     é€šè¿‡ {qdrant_pass}/{qdrant_total} â€” å¯èƒ½æ˜¯ Claude ç”¨é€šç”¨çŸ¥è¯†å›ç­”ï¼ˆéæ£€ç´¢ï¼‰", lf)
            log(f"     è®¾ç½® USE_MCP=1 å¯ç”¨ hybrid_search ä»¥æµ‹è¯•çœŸæ­£çš„å‘é‡æ£€ç´¢", lf)

        # LLM Judge ç»Ÿè®¡
        if USE_JUDGE:
            judge_scores = [r.get("judge_score") for r in results
                           if r.get("judge_score") is not None]
            if judge_scores:
                avg_score = sum(judge_scores) / len(judge_scores)
                avg_faith = sum(r.get("faithfulness", 0) for r in results
                               if r.get("faithfulness") is not None and r.get("faithfulness", -1) >= 0) / max(len(judge_scores), 1)
                avg_rel = sum(r.get("relevancy", 0) for r in results
                             if r.get("relevancy") is not None and r.get("relevancy", -1) >= 0) / max(len(judge_scores), 1)
                low_quality = [r for r in results if r.get("judge_score") is not None and r["judge_score"] < 3]
                log("", lf)
                log(f"ğŸ§‘â€âš–ï¸ LLM Judge ({len(judge_scores)} cases):", lf)
                log(f"  å¹³å‡ score: {avg_score:.2f}/5 | faithfulness: {avg_faith:.2f}/5 | relevancy: {avg_rel:.2f}/5", lf)
                if low_quality:
                    log(f"  âš ï¸ ä½è´¨é‡ (score<3): {len(low_quality)} ä¸ª", lf)
                    for r in low_quality[:5]:
                        log(f"    - {r['test_id']}: score={r['judge_score']} {r.get('query', '')[:40]}", lf)

        # ä¿å­˜æ±‡æ€» JSON
        kb_commit = get_kb_commit()
        out_dir = PROJECT_ROOT / "eval"
        out_file = out_dir / f"agentic_rag_v5_{timestamp}.json"

        # Judge æ±‡æ€»
        judge_summary = {}
        if USE_JUDGE:
            judge_scores = [r.get("judge_score") for r in results
                           if r.get("judge_score") is not None]
            if judge_scores:
                judge_summary = {
                    "count": len(judge_scores),
                    "avg_score": round(sum(judge_scores) / len(judge_scores), 2),
                    "low_quality_count": sum(1 for s in judge_scores if s < 3),
                }

        with open(out_file, "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(), "test_type": "agentic_rag_v5",
                "method": "claude_agent_sdk", "total": total,
                "passed": passed, "failed": failed, "errors": errors,
                "total_time": total_time, "total_cost": total_cost,
                "kb_commit": kb_commit,
                "eval_module": "eval_module.py (gate + quality + judge)",
                "category_stats": {c: {"total": s["t"], "passed": s["p"]} for c, s in cats.items()},
                "source_stats": {s: {"total": v["t"], "passed": v["p"]} for s, v in source_stats.items()},
                "judge_summary": judge_summary,
                "use_mcp": USE_MCP,
                "use_judge": USE_JUDGE,
                "results": results,
            }, f, indent=2, ensure_ascii=False)

        log(f"\nğŸ“ æ±‡æ€»: {out_file}", lf)
        log(f"ğŸ“‹ æ—¥å¿—: {log_path}", lf)
        log(f"ğŸ“ è¯¦ç»†: {detail_path}", lf)

    return results


if __name__ == "__main__":
    asyncio.run(main())
